{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Applied to Algorithmic Trading\n",
    "\n",
    "<a href=\"https://www.kaggle.com/addarm/unsupervised-learning-as-signals-for-pairs-trading\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRO\n",
    "\n",
    "\n",
    "This deep learning network was inspired by the paper:\n",
    "```BibTeX\n",
    "@article{theate2021application,\n",
    "  title={An application of deep reinforcement learning to algorithmic trading},\n",
    "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
    "  journal={Expert Systems with Applications},\n",
    "  volume={173},\n",
    "  pages={114632},\n",
    "  year={2021},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 13:20:21.897516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 13:20:21.897571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 13:20:21.898865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 13:20:21.907342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 13:20:22.908310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/adamd/workspace/deep-reinforced-learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1' # KERAS 2 only for tfagents\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install scikit-learn\n",
    "    %pip install tensorflow\n",
    "    %pip install tqdm\n",
    "    %pip install matplotlib\n",
    "    %pip install python-dotenv\n",
    "    %pip install yfinance\n",
    "    %pip install pyarrow\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
    "else:\n",
    "    DATA_DIR = \"./data/\"\n",
    "    print('Running Local...')\n",
    "\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import py_environment, utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2017-01-01\"\n",
    "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
    "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
    "DATA_DIR = \"./data\"\n",
    "INDEX = \"Date\"\n",
    "TARGET = 'TSLA'\n",
    "TICKER_SYMBOLS = [TARGET]\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "MODELS_PATH = './models'\n",
    "LOGS_PATH = './logs'\n",
    "\n",
    "ACT_LONG = 2\n",
    "ACT_HOLD = 1\n",
    "ACT_SHORT = 0\n",
    "\n",
    "FEATURES = [\"Close\", \"High\", \"Low\", \"Open\"]\n",
    "CAPITAL = 100\n",
    "STATE_LEN = 1\n",
    "FEES = 0.1 / 100\n",
    "FEATURES = len(FEATURES) # 4 dims: HLOC\n",
    "OBS_SPACE = (STATE_LEN)*FEATURES\n",
    "ACT_SPACE = 2\n",
    "\n",
    "BATCH_SIZE = 64 # OBS_SPACE * 100\n",
    "LEARN_RATE = 1e-3\n",
    "TOTAL_ITERS = 10 # 10000\n",
    "EPISODES = 10\n",
    "INIT_COLLECT = 10 # 100\n",
    "TOTAL_COLLECT = 1\n",
    "LOG_INTERVALS = 1 # 200\n",
    "TEST_INTERVALS = 2 # 1000\n",
    "MEMORY_LENGTH = OBS_SPACE * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA => min_date: 2017-01-03 00:00:00, max_date: 2019-12-30 00:00:00, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>14.324000</td>\n",
       "      <td>14.688667</td>\n",
       "      <td>14.064000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>88849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>14.316667</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>14.287333</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>168202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>15.094667</td>\n",
       "      <td>15.165333</td>\n",
       "      <td>14.796667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>88675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>15.128667</td>\n",
       "      <td>15.354000</td>\n",
       "      <td>15.030000</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>82918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>15.264667</td>\n",
       "      <td>15.461333</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>59692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>27.452000</td>\n",
       "      <td>28.134001</td>\n",
       "      <td>27.333332</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>199794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>27.890667</td>\n",
       "      <td>28.364668</td>\n",
       "      <td>27.512667</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>120820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>28.527332</td>\n",
       "      <td>28.898666</td>\n",
       "      <td>28.423332</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>159508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.020666</td>\n",
       "      <td>28.407333</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>149185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>28.586000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>27.284000</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>188796000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2017-01-03  14.324000  14.688667  14.064000  14.466000  14.466000   88849500\n",
       "2017-01-04  14.316667  15.200000  14.287333  15.132667  15.132667  168202500\n",
       "2017-01-05  15.094667  15.165333  14.796667  15.116667  15.116667   88675500\n",
       "2017-01-06  15.128667  15.354000  15.030000  15.267333  15.267333   82918500\n",
       "2017-01-09  15.264667  15.461333  15.200000  15.418667  15.418667   59692500\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2019-12-23  27.452000  28.134001  27.333332  27.948000  27.948000  199794000\n",
       "2019-12-24  27.890667  28.364668  27.512667  28.350000  28.350000  120820500\n",
       "2019-12-26  28.527332  28.898666  28.423332  28.729334  28.729334  159508500\n",
       "2019-12-27  29.000000  29.020666  28.407333  28.691999  28.691999  149185500\n",
       "2019-12-30  28.586000  28.600000  27.284000  27.646667  27.646667  188796000\n",
       "\n",
       "[753 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
    "    tickers = {}\n",
    "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
    "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    for symbol in tickers_symbols:\n",
    "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(cached_file_path):\n",
    "                df = pd.read_parquet(cached_file_path)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                assert len(df) > 0\n",
    "            else:\n",
    "                df = yf.download(\n",
    "                    symbol,\n",
    "                    start=START_DATE,\n",
    "                    end=END_DATE,\n",
    "                    progress=False,\n",
    "                    interval=INTERVAL,\n",
    "                )\n",
    "                assert len(df) > 0\n",
    "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
    "            min_date = df.index.min()\n",
    "            max_date = df.index.max()\n",
    "            nan_count = df[\"Close\"].isnull().sum()\n",
    "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
    "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
    "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
    "            print(\n",
    "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
    "            )\n",
    "            tickers[symbol] = df\n",
    "\n",
    "            if min_date > latest_start:\n",
    "                latest_start = min_date\n",
    "            if max_date < earliest_end:\n",
    "                earliest_end = max_date\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "    return tickers, latest_start, earliest_end\n",
    "\n",
    "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
    "tickers[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep Specs: TimeStep(\n",
      "{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])})\n",
      "Action Specs: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "Reward Specs: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Time step: TimeStep(\n",
      "{'step_type': array(0, dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([0.0323942 , 0.02664311, 0.01137912, 0.01306935], dtype=float32)})\n",
      "Next time step: TimeStep(\n",
      "{'step_type': array(1, dtype=int32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.03292707,  0.22160004,  0.0116405 , -0.27600172], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "class TradingEnv(py_environment.PyEnvironment):\n",
    "    \"\"\"\n",
    "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
    "\n",
    "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
    "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the stock market data.\n",
    "    - data_dim: Dimension of the data to be used for each observation.\n",
    "    - money: Initial capital to start trading.\n",
    "    - stateLength: Number of past observations to consider for the state.\n",
    "    - transactionCosts: Costs associated with trading actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_dim, money, stateLength, transactionCosts):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        # self.data = self.preprocess_data(data)\n",
    "        self.data = data\n",
    "        self.data_dim = data_dim\n",
    "        self.min_balance = -money//4\n",
    "        self.initial_balance = money\n",
    "        self.state_length = stateLength\n",
    "        self.transaction_cost = transactionCosts\n",
    "        self._episode_ended = False\n",
    "\n",
    "        self._batch_size = 1\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.data_dim,), dtype=np.float32, name='observation')\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def batched(self):\n",
    "        return False #True\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return None #self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, size):\n",
    "        self._batch_size = size\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        log_returns = np.log(df / df.shift(1))\n",
    "        normalized_data = (log_returns - log_returns.mean()) / log_returns.std()\n",
    "        normalized_data.dropna(inplace=True)\n",
    "        return normalized_data\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Provides the specification of the action space.\"\"\"\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.total_shares = 0\n",
    "        self.current_step = self.state_length\n",
    "        self._episode_ended = False\n",
    "        initial_observation = self._next_observation()\n",
    "        return ts.restart(initial_observation)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \"\"\"Generates the next observation based on the current step.\"\"\"\n",
    "        frame = self.data.iloc[self.current_step-self.state_length:self.current_step]\n",
    "        obs = frame[['Close', 'Low', 'High', 'Volume']].values[0]\n",
    "        obs = obs.flatten().astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Executes a trading action and returns the new state of the environment.\"\"\"\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        self.current_step += 1\n",
    "        reward = 0\n",
    "\n",
    "        if action == ACT_SHORT or action == ACT_LONG:\n",
    "            if self.total_shares > 0:\n",
    "                self.balance += self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "                reward = self.balance - self.initial_balance\n",
    "                self.total_shares = 0\n",
    "            if action == ACT_LONG:\n",
    "                self.position = 1\n",
    "                self.total_shares = self.balance // (current_price * (1 + self.transaction_cost))\n",
    "                self.balance -= self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "            elif action == ACT_SHORT:\n",
    "                self.position = -1\n",
    "\n",
    "        if self.balance < self.min_balance:\n",
    "            # bankrupt\n",
    "            done = True\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = self.current_step >= len(self.data)\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self._next_observation(), reward)\n",
    "        else:\n",
    "            return ts.transition(self._next_observation(), reward)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}')\n",
    "\n",
    "stock= tickers[TARGET]\n",
    "train_data = stock[stock.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
    "test_data = stock[stock.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
    "\n",
    "# train_env = TradingEnv(train_data, FEATURES, CAPITAL, STATE_LEN, FEES)\n",
    "train_env = suite_gym.load('CartPole-v0')\n",
    "utils.validate_py_environment(train_env, episodes=EPISODES)\n",
    "# test_env = TradingEnv(test_data, FEATURES, CAPITAL, STATE_LEN, FEES)\n",
    "test_env = suite_gym.load('CartPole-v0')\n",
    "utils.validate_py_environment(train_env, episodes=EPISODES//4)\n",
    "\n",
    "print(f\"TimeStep Specs: {train_env.time_step_spec()}\")\n",
    "print(f\"Action Specs: {train_env.action_spec()}\")\n",
    "print(f\"Reward Specs: {train_env.time_step_spec().reward}\")\n",
    "\n",
    "time_step = train_env.reset()\n",
    "print(f'Time step: {time_step}')\n",
    "action = np.array(ACT_HOLD, dtype=np.int32)\n",
    "next_time_step = train_env.step(action)\n",
    "print(f'Next time step: {next_time_step}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "2 models:\n",
    "- Policy Model: This is the primary model that the agent uses to make decisions or select actions based on the current state of the environment. The policy model is actively trained and updated throughout the training process based on the agent's experiences. In real-life applications, after the training phase is complete, the policy model is what gets deployed to make decisions or take actions in the given environment.\n",
    "- Target Model: The target model is used exclusively during the training phase to provide a stable target for the temporal difference (TD) error calculation, which is crucial for the stability of the Q-learning updates. The target model's weights are periodically synchronized with the policy model's weights but at a much slower rate. This delayed update helps to stabilize the learning process by making the target for the policy updates more consistent across training batches. The target model itself is not used for decision-making or action selection outside of the training context.\n",
    "\n",
    "Some notes on this 2 model arch:\n",
    "- Stability/Reducing Temporal Correlations: The agent learns a policy that maps states to actions by using a Q-function. This Q-function estimates the rewards by taking a certain action in a given state. The learning process continuously updates the Q-values based on new experiences. If the Q-function is constantly changing—as it would be when updates are made based on estimates from the same function—it can lead to unstable training dynamics. The estimates can become overly optimistic, and the learning process can diverge.\n",
    "- Target: The target network is a stable baseline for the policy network to compare against. While the policy network is frequently updated to reflect the latest learning, the target network's weights are updated less frequently. This slower update rate provides a fixed target for the policy network to aim for over multiple iterations, making the learning process more stable.\n",
    "\n",
    "In practice, the policy network is responsible for selecting actions during training and gameplay. Its weights are regularly updated to reflect the agent's learning. The target network, on the other hand, is used to generate the Q-value targets for the updates of the policy network. Every few steps, the weights from the policy network are copied to the target network, ensuring the target for the updates remains relatively stable but still gradually adapts to the improved policy. The policy model is used both during training (for learning and decision-making) and after training (for decision-making in the deployment environment).\n",
    "\n",
    "The target model is used during the training process only, to calculate stable target values for updating the policy model.\n",
    "After training is complete and the model is deployed in a real-world application, only the policy model is used to make decisions or take actions based on the learned policy. The target model's role ends with the completion of the training phase, as its primary purpose is to aid in the convergence and stability of the training process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRL Flow\n",
    "\n",
    "- Initialization: init policy network and the target network with the same architecture but separate parameters.\n",
    "- Data Preparation: Normalize input data using calculated coefficients to ensure consistency in scale.\n",
    "- Learning Process:\n",
    "    -At each training step, observe the current market state and process it through normalization.\n",
    "    - Select an action using the epsilon-greedy policy (a balance between exploration and exploitation) based on the current state.\n",
    "    - Execute the selected action in the simulated trading environment, observe the next state, and receive a reward based on the action's outcome.\n",
    "    - Store the experience (current state, action, reward, next state) in the replay memory.\n",
    "    - Sample a random batch of past experiences from the replay memory for learning to reduce correlation between consecutive learning steps.\n",
    "    - Use the policy network to predict Q-values for the current states and the target network to calculate the target Q-values for the next states.\n",
    "    - Update the policy network by minimizing the difference between its Q-value predictions and the target Q-values using backpropagation.\n",
    "    - Every few steps, update the target network's parameters with the policy network's parameters to gradually adapt the learning target.\n",
    "- Evaluation and Adjustment: Periodically test the trained policy network on a separate validation set or environment to evaluate performance.\n",
    "    -Repeat the learning and evaluation process for many episodes until the policy network stabilizes and performs satisfactorily.\n",
    "- Application Phase\n",
    "    - Model Deployment: Deploy the trained policy network in a real-world environment or a simulation that closely mimics real trading conditions.\n",
    "    - Real-time Operation: Observe the current market state and process it (normalization, etc.) as done during training.\n",
    "    - Use the trained policy network to select the action that maximizes expected rewards based on the current market state, leaning towards exploitation of the learned policy over exploration. Execute the selected action in the market (buy, sell, hold).\n",
    "- Continuous Learning:\n",
    "    - Repeat the learning process with new market data and experiences, possibly in a less frequent, offline manner.\n",
    "    - Update the policy and target networks as new data becomes available and as the market evolves to maintain or improve performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 13:20:24.655449: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.696292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.696468: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.700896: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.701057: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.701162: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.884333: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.884600: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.884636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-10 13:20:24.884866: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 13:20:24.884899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2246 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf_agents.policies.greedy_policy.GreedyPolicy object at 0x7f14805e7cd0>\n",
      "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7f14af349a50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 13:20:25.299693: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def create_q_network(env, fc_layer_params = (100, 50)):\n",
    "    env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "    def _dense_layer(num_units):\n",
    "        return tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "    dense_layers = [_dense_layer(num_units) for num_units in fc_layer_params]\n",
    "    q_values_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.03, maxval=0.03),\n",
    "        bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "    q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "    return q_net\n",
    "\n",
    "def create_agent(q_net, env, optimizer = tf.keras.optimizers.Adam(learning_rate=LEARN_RATE)):\n",
    "    env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    train_step_counter = tf.Variable(0)\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        env.time_step_spec(),\n",
    "        env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        epsilon_greedy = 1.,\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "    agent.initialize()\n",
    "    print(agent.policy)\n",
    "    print(agent.collect_policy)\n",
    "    return agent\n",
    "\n",
    "q_net = create_q_network(train_env)\n",
    "agent = create_agent(q_net, train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/default/server.cc:84] Shutting down replay server\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpv82fe3sy.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmpv82fe3sy\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 34215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
      " 'policy_info': (),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})\n",
      "('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n",
      "TimeStep(\n",
      "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.01501091, -0.03539657, -0.03818798,  0.04011324]],\n",
      "      dtype=float32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())\n",
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f14ae172dd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   0%|          | 0/10 [00:00<?, ?it/s][reverb/cc/client.cc:165] Sampler and server are owned by the same process (500904) so Table uniform_table is accessed directly without gRPC.\n",
      "Training Loop:  10%|█         | 1/10 [00:00<00:03,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 1.1626999378204346\n",
      "step = 2: loss = 1.1164699792861938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  20%|██        | 2/10 [00:00<00:04,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2: Average Return = 51.0\n",
      "step = 3: loss = 1.14453125\n",
      "step = 4: loss = 1.059468388557434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  40%|████      | 4/10 [00:01<00:02,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4: Average Return = 138.89999389648438\n",
      "step = 5: loss = 1.1991806030273438\n",
      "step = 6: loss = 1.1779873371124268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  60%|██████    | 6/10 [00:02<00:01,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6: Average Return = 185.5\n",
      "step = 7: loss = 1.1507821083068848\n",
      "step = 8: loss = 1.0611670017242432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  80%|████████  | 8/10 [00:04<00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 8: Average Return = 411.6000061035156\n",
      "step = 9: loss = 1.180173635482788\n",
      "step = 10: loss = 1.1315783262252808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop: 100%|██████████| 10/10 [00:06<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10: Average Return = 640.5999755859375\n",
      "\n",
      "Training completed. Mean Reward: 245.6167, Mean Loss: 1.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./models/dqn_trained.h5-1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TradingSimulator:\n",
    "    def __init__(self, env, eval_env, agent, episodes=EPISODES, batch_size=BATCH_SIZE, num_eval_episodes=TEST_INTERVALS, collect_steps_per_iteration=INIT_COLLECT, replay_buffer_max_length=MEMORY_LENGTH , num_iterations = TOTAL_ITERS, log_interval=LOG_INTERVALS, eval_interval=TEST_INTERVALS):\n",
    "        self.py_env = env\n",
    "        self.env =  tf_py_environment.TFPyEnvironment(self.py_env)\n",
    "        self.py_eval_env = eval_env\n",
    "        self.eval_env =  tf_py_environment.TFPyEnvironment(self.py_eval_env)\n",
    "        self.agent = agent\n",
    "        self.episodes = episodes\n",
    "        self.log_interval = log_interval\n",
    "        self.eval_interval = eval_interval\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "        self.collect_steps_per_iteration = collect_steps_per_iteration\n",
    "        self.replay_buffer_max_length = replay_buffer_max_length\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.policy = self.agent.policy\n",
    "        self.collect_policy = self.agent.collect_policy\n",
    "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
    "            self.env.time_step_spec(),\n",
    "            self.env.action_spec())\n",
    "\n",
    "        self.replay_buffer_signature = tensor_spec.from_spec(\n",
    "            self.agent.collect_data_spec)\n",
    "        self.replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "            self.replay_buffer_signature)\n",
    "\n",
    "    def compute_avg_return(self, environment, policy, num_eval_episodes):\n",
    "        total_return = 0.0\n",
    "        for _ in range(num_eval_episodes):\n",
    "            time_step = self.env.reset()\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = environment.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "                total_return += episode_return\n",
    "\n",
    "        avg_return = total_return / self.episodes\n",
    "        return avg_return.numpy()[0]\n",
    "\n",
    "    def init_memory(self, table_name = 'uniform_table'):\n",
    "        self.table = reverb.Table(\n",
    "            table_name,\n",
    "            max_size=self.replay_buffer_max_length,\n",
    "            sampler=reverb.selectors.Uniform(),\n",
    "            remover=reverb.selectors.Fifo(),\n",
    "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "            signature=self.replay_buffer_signature)\n",
    "\n",
    "        self.reverb_server = reverb.Server([self.table])\n",
    "        self.replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "                                    self.agent.collect_data_spec,\n",
    "                                    table_name=table_name,\n",
    "                                    sequence_length=2,\n",
    "                                    local_server=self.reverb_server)\n",
    "\n",
    "        self.rb_observer = reverb_utils.ReverbAddTrajectoryObserver(self.replay_buffer.py_client, table_name, sequence_length=2)\n",
    "\n",
    "        print(self.agent.collect_data_spec)\n",
    "        print(self.agent.collect_data_spec._fields)\n",
    "\n",
    "        # Random collections to init and test\n",
    "        py_driver.PyDriver(\n",
    "            self.py_env,\n",
    "            py_tf_eager_policy.PyTFEagerPolicy(self.random_policy, True),\n",
    "            [self.rb_observer],\n",
    "            max_steps=self.collect_steps_per_iteration).run(self.py_env.reset())\n",
    "\n",
    "        time_step = self.env.reset()\n",
    "        print(time_step)\n",
    "        print(self.random_policy.action(time_step))\n",
    "\n",
    "        self.compute_avg_return(self.eval_env, self.random_policy, self.num_eval_episodes)\n",
    "\n",
    "        self.dataset = self.replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=self.batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "        iterator = iter(self.dataset)\n",
    "        print(iterator)\n",
    "\n",
    "        return self.dataset, iterator\n",
    "\n",
    "    def train(self):\n",
    "        _, iterator = self.init_memory()\n",
    "\n",
    "        self.agent.train = common.function(self.agent.train)\n",
    "        self.agent.train_step_counter.assign(0)\n",
    "\n",
    "        # Agent's first eval\n",
    "        avg_return = self.compute_avg_return(self.eval_env, self.agent.policy, self.num_eval_episodes)\n",
    "        returns = [avg_return]\n",
    "        time_step = self.py_env.reset()\n",
    "        collect_driver = py_driver.PyDriver(\n",
    "            self.py_env,\n",
    "            py_tf_eager_policy.PyTFEagerPolicy(self.agent.collect_policy, use_tf_function=True),\n",
    "            [self.rb_observer],\n",
    "            max_steps=self.collect_steps_per_iteration)\n",
    "\n",
    "        for _ in tqdm(range(self.num_iterations), desc=\"Training Loop\"):\n",
    "            time_step, _ = collect_driver.run(time_step)\n",
    "            experience, _ = next(iterator)\n",
    "            train_loss = self.agent.train(experience).loss\n",
    "\n",
    "            step = self.agent.train_step_counter.numpy()\n",
    "\n",
    "            if step % self.log_interval == 0:\n",
    "                print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "            if step % self.eval_interval == 0:\n",
    "                avg_return = self.compute_avg_return(self.eval_env, self.agent.policy, self.num_eval_episodes)\n",
    "                print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "                returns.append(avg_return)\n",
    "\n",
    "        print(f'\\nTraining completed. Mean Reward: {np.mean(returns):.4f}, Mean Loss: {np.mean(train_loss):.4f}')\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def get_policy_eval(self, num_episodes=5):\n",
    "        results = []\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = self.eval_env.reset()\n",
    "            results.append(self.eval_py_env.render())\n",
    "            while not time_step.is_last():\n",
    "                action_step = self.random_policy.action(time_step)\n",
    "                time_step = self.eval_env.step(action_step.action)\n",
    "                results.append(self.py_eval_env.render())\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_performance(self, returns):\n",
    "        \"\"\"\n",
    "        Plot the training performance including rewards and losses.\n",
    "        \"\"\"\n",
    "        iterations = range(0, self.num_iterations + 1, self.eval_interval)\n",
    "        plt.plot(iterations, returns)\n",
    "        plt.ylabel('Average Return')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylim(top=250)\n",
    "\n",
    "sim = TradingSimulator(train_env, test_env, agent=agent)\n",
    "returns = sim.train()\n",
    "sim.plot_performance(returns)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=q_net)\n",
    "checkpoint.save(f'{MODELS_PATH}/dqn_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "CONCLUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
    "- [Open Gym AI Github](https://github.com/openai/gym)\n",
    "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
    "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
    "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
    "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
    "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github\n",
    "\n",
    "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
    "\n",
    "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/unsupervised-learning-as-signals-for-pairs-trading)\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
