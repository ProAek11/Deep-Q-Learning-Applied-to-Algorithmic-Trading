{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaDoHbxVH0CW"
      },
      "source": [
        "# Deep Q-Learning Applied to Algorithmic Trading\n",
        "\n",
        "<a href=\"https://www.kaggle.com/addarm/unsupervised-learning-as-signals-for-pairs-trading\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/drl_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM59cTClH0CZ"
      },
      "source": [
        "INTRO\n",
        "\n",
        "\n",
        "This deep learning network was inspired by the paper:\n",
        "```BibTeX\n",
        "@article{theate2021application,\n",
        "  title={An application of deep reinforcement learning to algorithmic trading},\n",
        "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
        "  journal={Expert Systems with Applications},\n",
        "  volume={173},\n",
        "  pages={114632},\n",
        "  year={2021},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4-GoceIIfT_",
        "outputId": "0f9e085b-7547-4aec-e35b-78283ec1a431"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "\n",
        "  !sudo apt-get update\n",
        "  !sudo apt-get install -y xvfb freeglut3-dev\n",
        "  !pip install pyvirtualdisplay\n",
        "  !pip install tf-agents[reverb]\n",
        "  !pip install pyglet\n",
        "  !pip install tf-keras\n",
        "  !pip install shutil\n",
        "\n",
        "  from google.colab import files\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  files = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C39UfWDmzYvL",
        "outputId": "6fe9803f-6fe5-45eb-e465-175ca00d6767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Local...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1' # KERAS 2 only for tfagents\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import shutil\n",
        "\n",
        "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
        "if IN_COLAB or IS_KAGGLE:\n",
        "    # Kaggle confgs\n",
        "    print('Running in Kaggle or Collab...')\n",
        "    %pip install scikit-learn\n",
        "    %pip install tensorflow\n",
        "    %pip install tqdm\n",
        "    %pip install matplotlib\n",
        "    %pip install python-dotenv\n",
        "    %pip install yfinance\n",
        "    %pip install pyarrow\n",
        "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))\n",
        "\n",
        "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
        "else:\n",
        "    DATA_DIR = \"./data/\"\n",
        "    print('Running Local...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mh5TWk3l1CJ-"
      },
      "outputs": [],
      "source": [
        "START_DATE = \"2017-01-01\"\n",
        "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
        "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
        "DATA_DIR = \"./data\"\n",
        "INDEX = \"Date\"\n",
        "TARGET = 'TSLA'\n",
        "TICKER_SYMBOLS = [TARGET]\n",
        "INTERVAL = \"1d\"\n",
        "\n",
        "MODELS_PATH = './models'\n",
        "LOGS_PATH = './logs'\n",
        "\n",
        "ACT_NEUTRAL = 3 # Added this action to go to neutral and wait\n",
        "ACT_LONG = 2\n",
        "ACT_HOLD = 1\n",
        "ACT_SHORT = 0\n",
        "\n",
        "CAPITAL = 100000\n",
        "TRADE_COSTS_PERCENT = 0.1 / 100\n",
        "\n",
        "FEATURES = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
        "TARGET_FEATURE = \"Price Raw\"\n",
        "STATE_LEN = 1\n",
        "OBS_SPACE = (STATE_LEN)*len(FEATURES)\n",
        "ACT_SPACE = 2\n",
        "\n",
        "BATCH_SIZE = OBS_SPACE * 1000\n",
        "LEARN_RATE = 1e-3\n",
        "TOTAL_ITERS = 100 # 10000\n",
        "EPISODES = 10\n",
        "INIT_COLLECT = 100\n",
        "TOTAL_COLLECT = 1\n",
        "LOG_INTERVALS = 20 # 200\n",
        "TEST_INTERVALS = 10 # 1000\n",
        "MEMORY_LENGTH = OBS_SPACE * 100\n",
        "DISCOUNT = 0.4\n",
        "EPSILON_START = 1.\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 10000\n",
        "ALPHA = 0.1\n",
        "L2Factor = 0.000001\n",
        "NEURONS = 512\n",
        "LAYERS = (NEURONS, NEURONS//2, NEURONS//4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4DpIfIPH0Ch"
      },
      "source": [
        "# Financial Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GJiIs_h-H0Ca",
        "outputId": "8fa7dad3-a997-497d-cd90-f2d483d2f02e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 14:40:31.895038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-12 14:40:31.895247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-12 14:40:31.899896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-12 14:40:32.025152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-12 14:40:35.245978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/mnt/c/Users/adamd/workspace/deep-reinforced-learning'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import shutil\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BDay\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "import tensorflow as tf\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym, py_environment, tf_py_environment, utils\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy, policy_saver, random_tf_policy\n",
        "\n",
        "import reverb\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "s64pmt9mH0Cj",
        "outputId": "7710cff6-9388-4bee-b6d5-2cb87e65da3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TSLA => min_date: 2017-01-03 00:00:00, max_date: 2019-12-30 00:00:00, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>14.324000</td>\n",
              "      <td>14.688667</td>\n",
              "      <td>14.064000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>88849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>14.316667</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>14.287333</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>168202500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>15.094667</td>\n",
              "      <td>15.165333</td>\n",
              "      <td>14.796667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>88675500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>15.128667</td>\n",
              "      <td>15.354000</td>\n",
              "      <td>15.030000</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>82918500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>15.264667</td>\n",
              "      <td>15.461333</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>59692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-23</th>\n",
              "      <td>27.452000</td>\n",
              "      <td>28.134001</td>\n",
              "      <td>27.333332</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>199794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-24</th>\n",
              "      <td>27.890667</td>\n",
              "      <td>28.364668</td>\n",
              "      <td>27.512667</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>120820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-26</th>\n",
              "      <td>28.527332</td>\n",
              "      <td>28.898666</td>\n",
              "      <td>28.423332</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>159508500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.020666</td>\n",
              "      <td>28.407333</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>149185500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>28.586000</td>\n",
              "      <td>28.600000</td>\n",
              "      <td>27.284000</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>188796000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>753 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Open       High        Low      Close  Adj Close     Volume\n",
              "Date                                                                        \n",
              "2017-01-03  14.324000  14.688667  14.064000  14.466000  14.466000   88849500\n",
              "2017-01-04  14.316667  15.200000  14.287333  15.132667  15.132667  168202500\n",
              "2017-01-05  15.094667  15.165333  14.796667  15.116667  15.116667   88675500\n",
              "2017-01-06  15.128667  15.354000  15.030000  15.267333  15.267333   82918500\n",
              "2017-01-09  15.264667  15.461333  15.200000  15.418667  15.418667   59692500\n",
              "...               ...        ...        ...        ...        ...        ...\n",
              "2019-12-23  27.452000  28.134001  27.333332  27.948000  27.948000  199794000\n",
              "2019-12-24  27.890667  28.364668  27.512667  28.350000  28.350000  120820500\n",
              "2019-12-26  28.527332  28.898666  28.423332  28.729334  28.729334  159508500\n",
              "2019-12-27  29.000000  29.020666  28.407333  28.691999  28.691999  149185500\n",
              "2019-12-30  28.586000  28.600000  27.284000  27.646667  27.646667  188796000\n",
              "\n",
              "[753 rows x 6 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
        "    tickers = {}\n",
        "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
        "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    for symbol in tickers_symbols:\n",
        "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(cached_file_path):\n",
        "                df = pd.read_parquet(cached_file_path)\n",
        "                df.index = pd.to_datetime(df.index)\n",
        "                assert len(df) > 0\n",
        "            else:\n",
        "                df = yf.download(\n",
        "                    symbol,\n",
        "                    start=START_DATE,\n",
        "                    end=END_DATE,\n",
        "                    progress=False,\n",
        "                    interval=INTERVAL,\n",
        "                )\n",
        "                assert len(df) > 0\n",
        "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
        "            min_date = df.index.min()\n",
        "            max_date = df.index.max()\n",
        "            nan_count = df[\"Close\"].isnull().sum()\n",
        "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
        "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
        "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
        "            print(\n",
        "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
        "            )\n",
        "            tickers[symbol] = df\n",
        "\n",
        "            if min_date > latest_start:\n",
        "                latest_start = min_date\n",
        "            if max_date < earliest_end:\n",
        "                earliest_end = max_date\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {symbol}: {e}\")\n",
        "\n",
        "    return tickers, latest_start, earliest_end\n",
        "\n",
        "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
        "tickers[TARGET]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lke4koO5H0Cl"
      },
      "source": [
        "# Trading Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXLhk-CKH0Co",
        "outputId": "143daa16-97dd-49ca-feb8-01680f06ecf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TimeStep Specs: TimeStep(\n",
            "{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n",
            " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
            " 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
            " 'observation': BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)})\n",
            "Action Specs: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n",
            "Reward Specs: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
            "Time step: TimeStep(\n",
            "{'step_type': array(0, dtype=int32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 1.9550354 ,  1.6273892 ,  0.6523866 , -0.08771443,  1.6908462 ],\n",
            "      dtype=float32)})\n",
            "Next time step: TimeStep(\n",
            "{'step_type': array(1, dtype=int32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 1.9550354 ,  1.6273892 ,  0.6523866 , -0.08771443,  1.6908462 ],\n",
            "      dtype=float32)})\n"
          ]
        }
      ],
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
        "\n",
        "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
        "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the stock market data.\n",
        "    - data_dim: Dimension of the data to be used for each observation.\n",
        "    - money: Initial capital to start trading.\n",
        "    - state_length: Number of past observations to consider for the state.\n",
        "    - transaction_cost: Costs associated with trading actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, features = FEATURES, money=CAPITAL, state_length=STATE_LEN, transaction_cost=0, market_costs=TRADE_COSTS_PERCENT):\n",
        "        super(TradingEnv, self).__init__()\n",
        "\n",
        "        assert data is not None\n",
        "\n",
        "        self.features = features\n",
        "        self.data_dim = len(self.features)\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.balance = money\n",
        "        self.initial_balance = money\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.epsilon = max(market_costs, np.finfo(float).eps) # there is always volatility costs\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self.state_length = state_length\n",
        "        self._episode_ended = False\n",
        "        self._batch_size = 1\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.data_dim,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.data = self.preprocess_data(data.copy())\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def batched(self):\n",
        "        return False #True\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return None #self._batch_size\n",
        "\n",
        "    @batch_size.setter\n",
        "    def batch_size(self, size):\n",
        "        self._batch_size = size\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        def _log_rets(df):\n",
        "            log_returns = np.log(df / df.shift(1))\n",
        "            df = (log_returns - log_returns.mean()) / log_returns.std()\n",
        "            df = df.dropna()\n",
        "            return df\n",
        "\n",
        "        price_raw = df['Close'].copy()\n",
        "        df[self.features] = _log_rets(df[self.features])\n",
        "        l = df[self.features]\n",
        "        df = df.replace(0.0, np.nan)\n",
        "        df = df.interpolate(method='linear', limit=5, limit_area='inside')\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        df[TARGET_FEATURE] = price_raw\n",
        "        df['Position'] = 0\n",
        "        df['Action'] = ACT_HOLD\n",
        "        df['Holdings'] = 0.\n",
        "        df['Cash'] = float(self.balance)\n",
        "        df['Money'] = df['Holdings'] + df['Cash']\n",
        "        df['Returns'] = 0.\n",
        "\n",
        "        assert not df.isna().any().any()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Provides the specification of the action space.\"\"\"\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_step = 0\n",
        "        self._episode_ended = False\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self.data['Position'] = 0\n",
        "        self.data['Action'] = ACT_HOLD\n",
        "        self.data['Holdings'] = 0.\n",
        "        self.data['Cash']  = float(self.balance)\n",
        "        self.data['Money'] = self.data.iloc[0]['Holdings'] + self.data.iloc[0]['Cash']\n",
        "        self.data['Returns'] = 0.\n",
        "\n",
        "        initial_observation = self._next_observation()\n",
        "        return ts.restart(initial_observation)\n",
        "\n",
        "    def _next_observation(self):\n",
        "        \"\"\"Generates the next observation based on the current step.\"\"\"\n",
        "        if self.current_step == 0 or self.state_length == 1:\n",
        "            obs = self.data[self.features].iloc[0: self.current_step + 1]\n",
        "        else:\n",
        "            obs = self.data[self.features].iloc[min(0, self.current_step-self.state_length):self.current_step]\n",
        "            assert len(obs) == self.state_length\n",
        "        obs = obs.values[0]\n",
        "        obs = obs.flatten().astype(np.float32)\n",
        "        return obs\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Executes a trading action and updates the environment's state.\"\"\"\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        self.current_step += 1\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        current_price = self.data.iloc[self.current_step][TARGET_FEATURE]\n",
        "\n",
        "        if self.current_step == 250 or  self.data.iloc[self.current_step].isna().any().any():\n",
        "            assert not self.data.iloc[self.current_step].isna().any().any()\n",
        "\n",
        "        if action == ACT_LONG:\n",
        "            self._process_long_position(current_price)\n",
        "        elif action == ACT_SHORT:\n",
        "            prev_current_price = self.data.iloc[self.current_step - 1][TARGET_FEATURE]\n",
        "            self._process_short_position(current_price, prev_current_price)\n",
        "        elif action == ACT_HOLD:\n",
        "            self._process_hold_position()\n",
        "        elif action == ACT_NEUTRAL:\n",
        "            self._process_neutral_position(current_price)\n",
        "        else:\n",
        "          raise Exception(f\"Invalid Actions: {action}\")\n",
        "\n",
        "        self._update_financials(current_price)\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        reward = self._calculate_reward()\n",
        "        if done:\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(self._next_observation(), reward)\n",
        "        else:\n",
        "            return ts.transition(self._next_observation(), reward)\n",
        "\n",
        "    def _get_lower_bound(self, cash, total_shares, price):\n",
        "        \"\"\"\n",
        "        Compute the lower bound of the action space, particularly for short selling,\n",
        "        based on current cash, the number of shares, and the current price.\n",
        "        \"\"\"\n",
        "        delta = -cash - total_shares * price * (1 + self.epsilon) * (1 + self.transaction_cost)\n",
        "\n",
        "        if delta < 0:\n",
        "            lowerBound = delta / (price * (2 * self.transaction_cost + self.epsilon * (1 + self.transaction_cost)))\n",
        "        else:\n",
        "            lowerBound = delta / (price * self.epsilon * (1 + self.transaction_cost))\n",
        "\n",
        "        if np.isinf(lowerBound):\n",
        "            assert False\n",
        "        return lowerBound\n",
        "\n",
        "    def _process_hold_position(self):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "        self.data.at[step_idx, \"Holdings\"] = self.data.iloc[self.current_step - 1][\"Holdings\"]\n",
        "        self.data.at[step_idx, \"Position\"] = self.data.iloc[self.current_step - 1][\"Position\"]\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
        "\n",
        "    def _process_neutral_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "        self.data.at[step_idx, \"Holdings\"] = 0.0\n",
        "        self.data.at[step_idx, \"Position\"] = 0.0\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_NEUTRAL\n",
        "\n",
        "    def _process_long_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = 1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == 1:\n",
        "            # more long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash']\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new long\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "        else:\n",
        "            # short to long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "\n",
        "    def _process_short_position(self, current_price, prev_price):\n",
        "        \"\"\"\n",
        "        Adjusts the logic for processing short positions to include lower bound calculations.\n",
        "        \"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = -1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == -1:\n",
        "            # Short more\n",
        "            low = self._get_lower_bound(self.data.iloc[self.current_step - 1]['Cash'], -self.total_shares, prev_price)\n",
        "            if low <= 0:\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            else:\n",
        "                total_sharesToBuy = min(math.floor(low), self.total_shares)\n",
        "                self.total_shares -= total_sharesToBuy\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] - total_sharesToBuy * current_price * (1 + self.transaction_cost)\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new short\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "        else:\n",
        "            # long to short\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "\n",
        "    def _update_financials(self, current_price):\n",
        "        \"\"\"Updates the financial metrics including cash, money, and returns.\"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "\n",
        "        self.data.at[step_idx,'Money'] = self.data.iloc[self.current_step]['Holdings'] + self.data.iloc[self.current_step]['Cash']\n",
        "        self.data.at[step_idx,'Returns'] = ((self.data.iloc[self.current_step]['Money'] - self.data.iloc[self.current_step - 1]['Money'])) / self.data.iloc[self.current_step - 1]['Money']\n",
        "\n",
        "    def _calculate_reward(self, reward_clip=1000):\n",
        "        \"\"\"\n",
        "        Calculates the reward for the current step.\n",
        "        \"\"\"\n",
        "        return np.clip(self.data.iloc[self.current_step]['Returns'], -reward_clip, reward_clip)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f'Step: {self.current_step}, Balance: {self.balance}')\n",
        "\n",
        "stock= tickers[TARGET]\n",
        "train_data = stock[stock.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
        "test_data = stock[stock.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
        "\n",
        "train_env = TradingEnv(train_data)\n",
        "utils.validate_py_environment(train_env, episodes=EPISODES)\n",
        "test_env = TradingEnv(test_data)\n",
        "utils.validate_py_environment(train_env, episodes=EPISODES//4)\n",
        "\n",
        "print(f\"TimeStep Specs: {train_env.time_step_spec()}\")\n",
        "print(f\"Action Specs: {train_env.action_spec()}\")\n",
        "print(f\"Reward Specs: {train_env.time_step_spec().reward}\")\n",
        "\n",
        "time_step = train_env.reset()\n",
        "print(f'Time step: {time_step}')\n",
        "action = np.array(ACT_HOLD, dtype=np.int32)\n",
        "next_time_step = train_env.step(action)\n",
        "print(f'Next time step: {next_time_step}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viOTgMvkPLkO"
      },
      "source": [
        "# The Problem Definition\n",
        "\n",
        "We are teaching an agent to trade in an environment with many unknowns. Our objective to make sequential interaction that lead to the highest sharpe ratio.\n",
        "\n",
        "Let's formulize our policy, which is finding the optimal action *a_t* given state *s_t* to maximize our expected reward *r_t*:\n",
        "\n",
        "$$\n",
        "\\pi^*(a_t | s_t) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\middle| s_t = f(o_1, a_1, r_1, \\ldots, o_t), a_t \\right]\n",
        "$$\n",
        "\n",
        "At each timestep *t*:\n",
        "\n",
        "1. Observe the environments state *s_t* and map history with *f(.)*\n",
        "2. Observations *o_t* from history *h_t*, have previous actions *a_t-1*, previous observations *o_t-1* and their returns *r_t-1*. For our experiment, we'll encode these into features for a network.\n",
        "3. Execute action *a_t*, which can be: hold, long, short\n",
        "4. Get returns *r_t* discounted at *γ_t*. *γ* is the discounting factor to prevent the agent from doing only tactical choices for returns in the present (missing better future returns).\n",
        "\n",
        "\n",
        "The *π(a_t|h_t)* creates an action on quantity *at = Qt*. Where a positive *Q* is the long, the negative *Q* signals a short and when its 0 no action is taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Roh_T5bs1c"
      },
      "source": [
        "## Actions and Rewards\n",
        "\n",
        "A core concept in RL is rewards engineering.\n",
        "\n",
        "Let's look at our action space *A* at time *t*:\n",
        "\n",
        "$$\n",
        "a_t = Q_t \\in \\{Q_{\\text{Long}, t}, Q_{\\text{Short}, t}\\}\n",
        "$$\n",
        "\n",
        "The action *Q_Long,t* is set to maximize returns on a buy, given our liquidity *vc_t* (the value *v* of our portfolio with cash remainng *c*) and purchasing *Q_long* at price *p* shares (transaction costs *C*) if we are not already long:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Long}, t} =\n",
        "\\begin{cases}\n",
        "\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Long}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The action *Q_Short,t* aims to convert a **negative** number of shares to returns (shorting is the borrowing of shares, therefore our *v_c* will be initially negative).\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} =\n",
        "\\begin{cases}\n",
        "-2n_t - \\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Short}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Note the *-2n* is an indication to sell twice, meaning not close a long position but open a short position, also for the *Qn* shares, we need to negate the amount we can buy, as its a short position. If we had no shares to start, then *-2(0)* will not have an effect save for the short amount:\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} = -\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)} \\right\\rfloor\n",
        "$$\n",
        "\n",
        "We need to have boundaries, as a short can have infinite loss:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Short}, t} = \\max\\{\\hat{Q}_{\\text{Short}, t}, Q_t\\}\n",
        "$$\n",
        "\n",
        "Given that our portfolio cannot fall into negative amounts, we need to model constraints.\n",
        "1. Cash value *vc_t* needs to be large enough to return to neutral *n_t=0*.\n",
        "2. To return to 0, we need to adjust for costs *C* of market volatility epsiloc *ϵ*.\n",
        "3. We redifine the action space permissable to ensure we can always return to neutral.\n",
        "\n",
        "$$\n",
        "v_{c,t+1} \\geq -n_{t+1} p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "The action space *A* is redefined as a set of acceptable values for *Q_t* between boundaries *Q-* and *Q+*:\n",
        "\n",
        "$$\n",
        "A = \\left\\{ Q_t \\in \\mathbb{Z} \\cap \\left[Q_t^-, Q_t^+\\right] \\right\\}\n",
        "$$\n",
        "\n",
        "Where the top boundary *Q+* is:\n",
        "$$\n",
        "Q_t^+ = \\frac{v_{c,t}}{p_t (1+C)}\n",
        "$$\n",
        "\n",
        "And the lower boundary *Q-* is (for both coming out of a long where delta *t* is positive, or reversing a short and incurring twice the costs with delta *t* in the negative):\n",
        "\n",
        "$$\n",
        "Q_t^- = \\begin{cases}\n",
        "    \\frac{\\Delta t}{p_t \\varepsilon (1 + C)} & \\text{if } \\Delta t \\geq 0, \\\\\n",
        "    \\frac{\\Delta t}{p_t (2C + \\varepsilon(1 + C))} & \\text{if } \\Delta t < 0,\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "with *delta t* being the in change of portfolio value in time:\n",
        "\n",
        "$$\n",
        " t_Δ = -v_{c,t} - n_t p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "In the above boundaries, the cost of trading is defined as:\n",
        "\n",
        "$$\n",
        "v_{c,t+1} = v_{c,t} - Q_t p_t - C |Q_t| p_t\n",
        "$$\n",
        "\n",
        "Where *C* is the percentage cost of the transaction given the absolute quantity *|Q_t|* of shares and their price *p_t*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GChhvmHGohQ2"
      },
      "source": [
        "## Agent's Objective\n",
        "\n",
        "As initially declared in this section, our agent's aim is to maximize the sharpe ratio:\n",
        "\n",
        "$$\n",
        "\\max_{\\pi} \\left( \\frac{E\\left[\\sum_{t=0}^{T} \\gamma^t r_t - R_f\\right]}{\\sqrt{\\mathrm{Var}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]}} \\right)\n",
        "$$\n",
        "\n",
        "which is just the maximization of:\n",
        "\n",
        "$$\n",
        "\\text{sharpe}= \\left( \\frac{\\bar{R} - R_f}{\\sigma} \\right)\n",
        "$$\n",
        "\n",
        "or the returns of the portfolio (annualized) minus the risk free rate (at the time of writing, 5%) divided by the volatility or standard deviation of the portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcb7hq6yH0Cq"
      },
      "source": [
        "# Deep Q-Network Architecure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIE_vPrH0Cr"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "2 models:\n",
        "- Policy Model: This is the primary model that the agent uses to make decisions or select actions based on the current state of the environment. The policy model is actively trained and updated throughout the training process based on the agent's experiences. In real-life applications, after the training phase is complete, the policy model is what gets deployed to make decisions or take actions in the given environment.\n",
        "- Target Model: The target model is used exclusively during the training phase to provide a stable target for the temporal difference (TD) error calculation, which is crucial for the stability of the Q-learning updates. The target model's weights are periodically synchronized with the policy model's weights but at a much slower rate. This delayed update helps to stabilize the learning process by making the target for the policy updates more consistent across training batches. The target model itself is not used for decision-making or action selection outside of the training context.\n",
        "\n",
        "Some notes on this 2 model arch:\n",
        "- Stability/Reducing Temporal Correlations: The agent learns a policy that maps states to actions by using a Q-function. This Q-function estimates the rewards by taking a certain action in a given state. The learning process continuously updates the Q-values based on new experiences. If the Q-function is constantly changing—as it would be when updates are made based on estimates from the same function—it can lead to unstable training dynamics. The estimates can become overly optimistic, and the learning process can diverge.\n",
        "- Target: The target network is a stable baseline for the policy network to compare against. While the policy network is frequently updated to reflect the latest learning, the target network's weights are updated less frequently. This slower update rate provides a fixed target for the policy network to aim for over multiple iterations, making the learning process more stable.\n",
        "\n",
        "In practice, the policy network is responsible for selecting actions during training and gameplay. Its weights are regularly updated to reflect the agent's learning. The target network, on the other hand, is used to generate the Q-value targets for the updates of the policy network. Every few steps, the weights from the policy network are copied to the target network, ensuring the target for the updates remains relatively stable but still gradually adapts to the improved policy. The policy model is used both during training (for learning and decision-making) and after training (for decision-making in the deployment environment).\n",
        "\n",
        "The target model is used during the training process only, to calculate stable target values for updating the policy model.\n",
        "After training is complete and the model is deployed in a real-world application, only the policy model is used to make decisions or take actions based on the learned policy. The target model's role ends with the completion of the training phase, as its primary purpose is to aid in the convergence and stability of the training process itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q49V1zseH0Cs"
      },
      "source": [
        "## DRL Flow\n",
        "\n",
        "- Initialization: init policy network and the target network with the same architecture but separate parameters.\n",
        "- Data Preparation: Normalize input data using calculated coefficients to ensure consistency in scale.\n",
        "- Learning Process:\n",
        "    -At each training step, observe the current market state and process it through normalization.\n",
        "    - Select an action using the epsilon-greedy policy (a balance between exploration and exploitation) based on the current state.\n",
        "    - Execute the selected action in the simulated trading environment, observe the next state, and receive a reward based on the action's outcome.\n",
        "    - Store the experience (current state, action, reward, next state) in the replay memory.\n",
        "    - Sample a random batch of past experiences from the replay memory for learning to reduce correlation between consecutive learning steps.\n",
        "    - Use the policy network to predict Q-values for the current states and the target network to calculate the target Q-values for the next states.\n",
        "    - Update the policy network by minimizing the difference between its Q-value predictions and the target Q-values using backpropagation.\n",
        "    - Every few steps, update the target network's parameters with the policy network's parameters to gradually adapt the learning target.\n",
        "- Evaluation and Adjustment: Periodically test the trained policy network on a separate validation set or environment to evaluate performance.\n",
        "    -Repeat the learning and evaluation process for many episodes until the policy network stabilizes and performs satisfactorily.\n",
        "- Application Phase\n",
        "    - Model Deployment: Deploy the trained policy network in a real-world environment or a simulation that closely mimics real trading conditions.\n",
        "    - Real-time Operation: Observe the current market state and process it (normalization, etc.) as done during training.\n",
        "    - Use the trained policy network to select the action that maximizes expected rewards based on the current market state, leaning towards exploitation of the learned policy over exploration. Execute the selected action in the market (buy, sell, hold).\n",
        "- Continuous Learning:\n",
        "    - Repeat the learning process with new market data and experiences, possibly in a less frequent, offline manner.\n",
        "    - Update the policy and target networks as new data becomes available and as the market evolves to maintain or improve performance over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y5vgYpxH0Cs",
        "outputId": "9fd11411-5b1c-48bc-e585-aca38f5ff4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 14:40:57.006684: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.504858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.505023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.518117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.518275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.518360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.921759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.922347: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.922381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2024-03-12 14:40:57.922854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-03-12 14:40:57.923019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2246 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
            "2024-03-12 14:40:58.997066: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tf_agents.policies.greedy_policy.GreedyPolicy object at 0x7f3f5050f310>\n",
            "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7f3f50e48150>\n"
          ]
        }
      ],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def create_q_network(env, fc_layer_params = LAYERS):\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "    action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "    def _dense_layer(num_units):\n",
        "        return tf.keras.layers.Dense(\n",
        "            num_units,\n",
        "            activation=tf.keras.activations.relu,\n",
        "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "    dense_layers = [_dense_layer(num_units) for num_units in fc_layer_params]\n",
        "    q_values_layer = tf.keras.layers.Dense(\n",
        "        num_actions,\n",
        "        activation=None,\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(), # Xavier\n",
        "        bias_initializer=tf.keras.initializers.GlorotNormal())\n",
        "    q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "    return q_net\n",
        "\n",
        "def create_agent(q_net, env, t_q_net = None, train_step_counter=None, optimizer = tf.keras.optimizers.Adam(learning_rate=LEARN_RATE), eps=EPSILON_START, gradient_clipping = 1.):\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "    if train_step_counter is None:\n",
        "      train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "    # see: https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DqnAgent\n",
        "    agent = dqn_agent.DqnAgent(\n",
        "        env.time_step_spec(),\n",
        "        env.action_spec(),\n",
        "        q_network=q_net,\n",
        "        target_q_network = t_q_net,\n",
        "        optimizer=optimizer,\n",
        "        epsilon_greedy = 0.5,\n",
        "        reward_scale_factor = 0.01,\n",
        "        gradient_clipping = gradient_clipping,\n",
        "        td_errors_loss_fn=common.element_wise_huber_loss,\n",
        "        train_step_counter=train_step_counter,\n",
        "        name=\"TradeAgent\")\n",
        "\n",
        "    agent.initialize()\n",
        "    print(agent.policy)\n",
        "    print(agent.collect_policy)\n",
        "    return agent, train_step_counter\n",
        "\n",
        "q_net = create_q_network(train_env)\n",
        "t_q_net = create_q_network(train_env)\n",
        "agent, train_step_counter = create_agent(q_net, train_env, t_q_net=t_q_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZ_bo04H0Ct"
      },
      "source": [
        "# Trading Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7DmbtWugH0Ct"
      },
      "outputs": [],
      "source": [
        "class TradingSimulator:\n",
        "    def __init__(self, env, eval_env, agent, episodes=EPISODES,\n",
        "                 batch_size=BATCH_SIZE, num_eval_episodes=TEST_INTERVALS,\n",
        "                 collect_steps_per_iteration=INIT_COLLECT,\n",
        "                 replay_buffer_max_length=MEMORY_LENGTH ,\n",
        "                 num_iterations = TOTAL_ITERS, log_interval=LOG_INTERVALS,\n",
        "                 eval_interval=TEST_INTERVALS, global_step=None):\n",
        "        self.py_env = env\n",
        "        self.env =  tf_py_environment.TFPyEnvironment(self.py_env)\n",
        "        self.py_eval_env = eval_env\n",
        "        self.eval_env =  tf_py_environment.TFPyEnvironment(self.py_eval_env)\n",
        "        self.agent = agent\n",
        "        self.episodes = episodes\n",
        "        self.log_interval = log_interval\n",
        "        self.eval_interval = eval_interval\n",
        "        self.global_step = global_step\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_eval_episodes = num_eval_episodes\n",
        "        self.collect_steps_per_iteration = collect_steps_per_iteration\n",
        "        self.replay_buffer_max_length = replay_buffer_max_length\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "        self.policy = self.agent.policy\n",
        "        self.collect_policy = self.agent.collect_policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
        "            self.env.time_step_spec(),\n",
        "            self.env.action_spec())\n",
        "\n",
        "        self.replay_buffer_signature = tensor_spec.from_spec(\n",
        "            self.agent.collect_data_spec)\n",
        "        self.replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "            self.replay_buffer_signature)\n",
        "\n",
        "    def init_memory(self, table_name = 'uniform_table'):\n",
        "        self.table = reverb.Table(\n",
        "            table_name,\n",
        "            max_size=self.replay_buffer_max_length,\n",
        "            sampler=reverb.selectors.Uniform(),\n",
        "            remover=reverb.selectors.Fifo(),\n",
        "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "            signature=self.replay_buffer_signature)\n",
        "\n",
        "        self.reverb_server = reverb.Server([self.table])\n",
        "        self.replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "                                    self.agent.collect_data_spec,\n",
        "                                    table_name=table_name,\n",
        "                                    sequence_length=2,\n",
        "                                    local_server=self.reverb_server)\n",
        "\n",
        "        self.rb_observer = reverb_utils.ReverbAddTrajectoryObserver(self.replay_buffer.py_client, table_name, sequence_length=2)\n",
        "\n",
        "        print(self.agent.collect_data_spec)\n",
        "        print(self.agent.collect_data_spec._fields)\n",
        "\n",
        "        # Test with random actions\n",
        "        py_driver.PyDriver(\n",
        "            self.py_env,\n",
        "            py_tf_eager_policy.PyTFEagerPolicy(self.random_policy, True),\n",
        "            [self.rb_observer],\n",
        "            max_steps=self.collect_steps_per_iteration).run(self.py_env.reset())\n",
        "        time_step = self.env.reset()\n",
        "        for i in range(10):\n",
        "          action_step = self.random_policy.action(time_step)\n",
        "          time_step = self.env.step(action_step.action)\n",
        "          if time_step.is_last():\n",
        "              break\n",
        "\n",
        "        total_returns, avg_return, std_dev, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.random_policy, 3)\n",
        "        print(f'\\nRandom Policy Test: Mean Reward: {avg_return}, Mean Totals: {total_returns}, Mean Sharpe: {sharpe_ratio}')\n",
        "\n",
        "        self.dataset = self.replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=self.batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "        return self.dataset, iter(self.dataset)\n",
        "\n",
        "    def compute_episode_metrics(self, environment, policy, num_eval_episodes):\n",
        "        total_returns = []\n",
        "        episode_sharpe_ratios = []\n",
        "        episode_std_devs = []\n",
        "        episode_avg_returns = []\n",
        "\n",
        "        for _ in tqdm(range(num_eval_episodes), desc=f\"compute_episode_metrics for {num_eval_episodes}\"):\n",
        "            time_step = environment.reset()\n",
        "            episode_returns = []\n",
        "\n",
        "            while not time_step.is_last():\n",
        "                action_step = policy.action(time_step)\n",
        "                time_step = environment.step(action_step.action)\n",
        "                rewards = time_step.reward.numpy()\n",
        "                episode_returns.extend(rewards.flatten())\n",
        "\n",
        "            total_episode_return = np.sum(episode_returns)\n",
        "            total_returns.append(total_episode_return)\n",
        "            episode_avg_return = np.mean(episode_returns)\n",
        "            episode_avg_returns.append(episode_avg_return)\n",
        "            episode_std_dev = np.std(episode_returns)\n",
        "            episode_std_devs.append(episode_std_dev)\n",
        "            episode_sharpe_ratio = episode_avg_return / episode_std_dev if episode_std_dev > 0 else 0\n",
        "            episode_sharpe_ratios.append(episode_sharpe_ratio)\n",
        "\n",
        "        return np.mean(total_returns), np.mean(episode_avg_returns), np.mean(episode_std_devs), np.mean(episode_sharpe_ratios)\n",
        "\n",
        "    def train(self, checkpoint_path=MODELS_PATH, initial_epsilon= EPSILON_START, final_epsilon = EPSILON_END, decay_steps=EPSILON_DECAY):\n",
        "        print(\"Preparing replay memory and dataset\")\n",
        "        _, iterator = self.init_memory()\n",
        "        if self.global_step is None:\n",
        "          self.global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "        checkpoint_dir = os.path.join(checkpoint_path, 'checkpoint')\n",
        "        train_checkpointer = common.Checkpointer(\n",
        "            ckpt_dir=checkpoint_dir,\n",
        "            max_to_keep=1,\n",
        "            agent=agent,\n",
        "            policy=agent.policy,\n",
        "            replay_buffer=self.replay_buffer,\n",
        "            global_step=self.global_step\n",
        "        )\n",
        "        train_checkpointer.initialize_or_restore()\n",
        "        self.global_step = tf.compat.v1.train.get_global_step()\n",
        "\n",
        "        self.agent.train = common.function(self.agent.train)\n",
        "        self.agent.train_step_counter.assign(self.global_step )\n",
        "\n",
        "        print(\"Performing initial evaluations\")\n",
        "        # Agent's first eval\n",
        "        total_returns, avg_return, std_dev, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.agent.policy, 5)\n",
        "        print(f'baseline step: Average Return = {avg_return}, Total Return = {total_returns}, Sharpe = {sharpe_ratio}')\n",
        "        metrics = [[1, total_returns, avg_return, std_dev, sharpe_ratio]]\n",
        "\n",
        "        time_step = self.py_env.reset()\n",
        "        collect_driver = py_driver.PyDriver(\n",
        "            self.py_env,\n",
        "            py_tf_eager_policy.PyTFEagerPolicy(self.agent.collect_policy, use_tf_function=True),\n",
        "            [self.rb_observer],\n",
        "            max_steps=self.collect_steps_per_iteration)\n",
        "\n",
        "        print(\"Running training\")\n",
        "        for _ in tqdm(range(self.num_iterations), desc=f\"Training for {self.num_iterations}\"):\n",
        "            time_step, _ = collect_driver.run(time_step)\n",
        "            experience, _ = next(iterator)\n",
        "            train_loss = self.agent.train(experience).loss\n",
        "            step = self.agent.train_step_counter.numpy()\n",
        "\n",
        "            if step % self.log_interval == 0:\n",
        "                print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "                train_checkpointer.save(self.global_step)\n",
        "                self.zip_directories(self, checkpoint_path)\n",
        "\n",
        "            if step % self.eval_interval == 0:\n",
        "                total_returns, avg_return, std_dev, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.agent.policy, self.num_eval_episodes)\n",
        "                print('step = {0}: Average Return = {1}, Total Return = {2}, Sharpe = {3}'.format(step, avg_return, total_returns, sharpe_ratio))\n",
        "                metrics.append([train_loss, total_returns, avg_return, std_dev, sharpe_ratio])\n",
        "\n",
        "            # Later call: saved_policy = tf.saved_model.load(policy_dir)\n",
        "            train_checkpointer.save(self.global_step)\n",
        "\n",
        "            # This is epsilon decay\n",
        "            decayed_epsilon = final_epsilon + (initial_epsilon - final_epsilon) * \\\n",
        "                      np.exp(-1. * step / decay_steps)\n",
        "            agent.collect_policy._epsilon =decayed_epsilon\n",
        "\n",
        "        loss, totals, average, _, sharpe_ratios = zip(*metrics)\n",
        "        print(f'\\nTraining completed. Mean Reward: {np.mean(average):.4f}, Mean Totals: {np.mean(totals):.4f}, Mean Loss: {np.mean(loss):.4f}, Mean Sharpe: {np.mean(sharpe_ratios):.4f}')\n",
        "\n",
        "        policy_dir = os.path.join(checkpoint_path, 'policy')\n",
        "        tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "        tf_policy_saver.save(policy_dir)\n",
        "        self.zip_directories(self, checkpoint_path)\n",
        "        print(\"Policy saved\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def clear_directories(self, directories = [MODELS_PATH]):\n",
        "        for dir_path in directories:\n",
        "            try:\n",
        "                shutil.rmtree(dir_path)\n",
        "                print(f\"Successfully cleared {dir_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error clearing {dir_path}: {e}\")\n",
        "\n",
        "    def zip_directories(self, directories, output_filename = [f\"{MODELS_PATH}/model_files\"]):\n",
        "        with shutil.make_archive(output_filename, 'zip') as archive:\n",
        "            for dir_path in directories:\n",
        "                shutil.move(dir_path, archive)\n",
        "        print(f\"Archived directories into {output_filename}.zip\")\n",
        "        if IN_COLAB:\n",
        "            self.upload_to_drive(f\"{output_filename}.zip\", f\"{output_filename}.zip\")\n",
        "\n",
        "    def upload_to_drive(self, file_path, destination_path):\n",
        "        \"\"\"\n",
        "        Uploads a file to Google Drive. This function assumes that your Google Drive is mounted at '/content/drive'.\n",
        "\n",
        "        Parameters:\n",
        "        - file_path: The path to the file you want to upload.\n",
        "        - destination_path: The destination path in your Google Drive.\n",
        "        \"\"\"\n",
        "        assert IN_COLAB\n",
        "        shutil.copy(file_path, destination_path)\n",
        "        print(f\"File {file_path} uploaded to {destination_path}\")\n",
        "\n",
        "    def plot_performance(self, metrics):\n",
        "        \"\"\"\n",
        "        Plot the training performance including average returns and Sharpe Ratios on the same plot,\n",
        "        with returns on the left y-axis and Sharpe Ratios on the right y-axis.\n",
        "        \"\"\"\n",
        "        loss, _, average_returns, _, sharpe_ratios = zip(*metrics)  # Ignore std deviations as per your request\n",
        "        iterations = range(0, self.num_iterations + 1, self.eval_interval)\n",
        "        iterations = list(iterations)[:len(average_returns)]\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(18, 4))\n",
        "\n",
        "        axs[0].set_xlabel('Iterations')\n",
        "        axs[0].set_ylabel('Average Return')\n",
        "        axs[0].plot(iterations, average_returns, label='Average Return')\n",
        "        axs[0].tick_params(axis='y')\n",
        "\n",
        "        ax12 = axs[0].twinx()\n",
        "        ax12.set_ylabel('Sharpe Ratio')\n",
        "        ax12.plot(iterations, sharpe_ratios, label='Sharpe Ratio')\n",
        "        ax12.tick_params(axis='y')\n",
        "\n",
        "        axs[1].set_xlabel('Iterations')\n",
        "        axs[1].set_ylabel('Loss')\n",
        "        axs[1].plot(iterations, loss, label='Loss')\n",
        "        axs[1].tick_params(axis='y')\n",
        "\n",
        "        fig.tight_layout()\n",
        "        plt.title('Training Performance: Average Returns and Sharpe Ratios')\n",
        "        plt.show()\n",
        "sim = TradingSimulator(train_env, test_env, agent=agent, global_step=train_step_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment this for a new run to remove remnants\n",
        "sim.clear_directories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C8944WF9ykML",
        "outputId": "a7709b1d-bbef-4377-8893-a22e97373c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing replay memory and dataset\n",
            "Trajectory(\n",
            "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
            " 'observation': BoundedTensorSpec(shape=(5,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
            " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n",
            " 'policy_info': (),\n",
            " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
            " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
            " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})\n",
            "('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmp2_9jqg85.\n",
            "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmp2_9jqg85\n",
            "[reverb/cc/platform/default/server.cc:71] Started replay server on port 41621\n",
            "compute_episode_metrics for 3: 100%|██████████| 3/3 [00:24<00:00,  8.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Random Policy Test: Mean Reward: -0.0014501367695629597, Mean Totals: -0.726518452167511, Mean Sharpe: -0.04629862308502197\n",
            "Performing initial evaluations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 5: 100%|██████████| 5/5 [01:10<00:00, 14.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline step: Average Return = -0.00025685076252557337, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n",
            "Running training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training for 100:   0%|          | 0/100 [00:00<?, ?it/s][reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n",
            "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n",
            "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n",
            "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n",
            "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n",
            "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (48480) so Table uniform_table is accessed directly without gRPC.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/radmada/miniconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/radmada/miniconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "2024-03-12 14:42:50.542786: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3e57c98db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-03-12 14:42:50.542862: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
            "2024-03-12 14:42:50.621642: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-03-12 14:42:50.720833: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1710250970.985588   48762 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [02:15<00:00, 13.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 110: Average Return = -0.00025685079162940383, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training for 100:  19%|█▉        | 19/100 [03:04<03:52,  2.88s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 120: loss = 39.456817626953125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [02:43<00:00, 16.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 120: Average Return = -0.00025685079162940383, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [02:07<00:00, 12.72s/it]\n",
            "Training for 100:  30%|███       | 30/100 [08:16<48:23, 41.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 130: Average Return = -0.00025685079162940383, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training for 100:  39%|███▉      | 39/100 [08:28<03:11,  3.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 140: loss = 55.44098663330078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [01:41<00:00, 10.15s/it]\n",
            "Training for 100:  40%|████      | 40/100 [10:12<33:22, 33.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 140: Average Return = -0.00025685079162940383, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [02:00<00:00, 12.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 150: Average Return = 0.0, Total Return = 0.0, Sharpe = 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training for 100:  59%|█████▉    | 59/100 [12:47<02:31,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 160: loss = 107.70860290527344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "compute_episode_metrics for 10: 100%|██████████| 10/10 [02:28<00:00, 14.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 160: Average Return = -0.00025685079162940383, Total Return = -0.1286822259426117, Sharpe = -0.0083418944850564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training for 100:  63%|██████▎   | 63/100 [15:25<11:07, 18.05s/it]"
          ]
        }
      ],
      "source": [
        "metrics = sim.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM6Lz_mSylZw"
      },
      "outputs": [],
      "source": [
        "sim.plot_performance(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeb9dL6H0Cu"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "CONCLUDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrMDUgZH0Cv"
      },
      "source": [
        "## References\n",
        "\n",
        "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
        "- [Open Gym AI Github](https://github.com/openai/gym)\n",
        "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
        "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
        "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
        "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
        "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
        "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgjl92lH0Cv"
      },
      "source": [
        "## Github\n",
        "\n",
        "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
        "\n",
        "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/unsupervised-learning-as-signals-for-pairs-trading)\n",
        "\n",
        "## Media\n",
        "\n",
        "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
        "\n",
        "## CC Licensing and Use\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
