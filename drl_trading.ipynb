{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaDoHbxVH0CW"
      },
      "source": [
        "# Deep Q-Learning Applied to Algorithmic Trading\n",
        "\n",
        "<a href=\"https://www.kaggle.com/code/addarm/deep-q-rl-with-algorithmic-trading-policy\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1FTj65b2DA8oFgvmmjLc0XIwtII32PwcM?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fopVWzH_sxr-"
      },
      "source": [
        "![\"Pavlov's trader dog, DALEE 2024\"](https://github.com/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/images/rl_banner.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqvaSLkfsxr-"
      },
      "source": [
        "In the book *\"A Random Walk Down Wall Street\"*, the author Burton G. Malkiel claimed that: “a blindfolded monkey throwing darts at a newspaper's financial pages could select a portfolio that would do just as well as one carefully selected by experts.”.\n",
        "\n",
        "What if instead of monkey, it was Pavlov's dog trained with reinforcement learning to select the optimal portfolio strategy? In this article, Reinforcement Learning (RL) is the one used in machine learning, where an agent learns actions in an environment that maximizes their value. The agent learns from the outcomes of its actions, without being explicitly programmed with task-specific rules,\n",
        "\n",
        "The goal of any RL algo, is to find value-maximizing policy (*π*):\n",
        "$$\n",
        "\\pi* = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
        "$$\n",
        "Where *γ (0 ≤ γ ≤ 1)* is the discounting factor to control the agent's rewards, *t* is a timestep and *R* is the returns in that step. The policy in RL is the probability of taking **action *a*** in **state *s***.\n",
        "\n",
        "The algo we will adopt is **Q-Learning**, a model-free RL algorithm, that aims to indirectly learn the policy through the **VALUE** of an action for a discrete state its in, rather the policy itself. It's useful in our case, as it doesn't need to model the environment - in our case, the random capital markets.\n",
        "\n",
        "Estimating the **Q-Value** is done through the Bellman's equation:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a]\n",
        "$$\n",
        "\n",
        "These Q-values are placed in the Q-Tables and used by the agent as look-up, to find all possible actions' Q-values from the current state, and choose the action with the highest Q-value (exploitation). This is good within a finite space, but not in a stochastic environment with limitless combinations, a problem which we will solve with our neural-network.\n",
        "\n",
        "This agent designed in this article has been inspired by the paper of *Théate, Thibaut and Ernst, Damien (2021)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM59cTClH0CZ"
      },
      "source": [
        "```BibTeX\n",
        "@article{theate2021application,\n",
        "  title={An application of deep reinforcement learning to algorithmic trading},\n",
        "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
        "  journal={Expert Systems with Applications},\n",
        "  volume={173},\n",
        "  pages={114632},\n",
        "  year={2021},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4-GoceIIfT_",
        "outputId": "be8ad1c1-c5e7-4ee0-e75a-fb72a1e3286f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [Connected to cloud.r-\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [736 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,848 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,352 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [80.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.3 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,079 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,569 kB]\n",
            "Fetched 7,042 kB in 3s (2,544 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Collecting typing-extensions==4.5.0 (from tf-agents[reverb])\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting pygame==2.1.3 (from tf-agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697630 sha256=42a29d1e9b6cc437c47ea838711d1b55604ab7028172e9569a66a20fb2b94eb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: typing-extensions, rlds, pygame, gym, tf-agents, dm-reverb\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.6.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dm-reverb-0.14.0 gym-0.23.0 pygame-2.1.3 rlds-0.1.8 tf-agents-0.19.0 typing-extensions-4.5.0\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n",
            "Mounted on COLLAB\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import files\n",
        "  from google.colab import drive\n",
        "\n",
        "  !apt-get update\n",
        "  !pip install tf-agents[reverb]\n",
        "  !pip install tf-keras\n",
        "  !pip install shutil\n",
        "\n",
        "  IN_COLAB = True\n",
        "  GDRIVE = '/content/drive'\n",
        "  drive.mount(GDRIVE)\n",
        "  print(\"Mounted on COLLAB\")\n",
        "except:\n",
        "  GDRIVE = None\n",
        "  IN_COLAB = False\n",
        "  files = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C39UfWDmzYvL",
        "outputId": "0f59318c-5b1c-40ca-9d7b-cd8ce015b027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Local...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1' # KERAS 2 only for tfagents\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "    print('Running in Kaggle...')\n",
        "    %pip install scikit-learn\n",
        "    %pip install tensorflow\n",
        "    %pip install tqdm\n",
        "    %pip install matplotlib\n",
        "    %pip install python-dotenv\n",
        "    %pip install yfinance\n",
        "    %pip install pyarrow\n",
        "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))\n",
        "\n",
        "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
        "    IN_KAGGLE = True\n",
        "else:\n",
        "    IN_KAGGLE = False\n",
        "    DATA_DIR = \"./data/\"\n",
        "    print('Running Local...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GJiIs_h-H0Ca",
        "outputId": "ec5de372-45db-40e2-a851-fab2212c02f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import shutil\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BDay\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "import tensorflow as tf\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym, py_environment, tf_py_environment, utils\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy, policy_saver, random_tf_policy\n",
        "\n",
        "import reverb\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh5TWk3l1CJ-"
      },
      "outputs": [],
      "source": [
        "START_DATE = \"2017-01-01\"\n",
        "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
        "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
        "DATA_DIR = \"./data\"\n",
        "INDEX = \"Date\"\n",
        "TARGET = 'TSLA'\n",
        "TICKER_SYMBOLS = [TARGET]\n",
        "INTERVAL = \"1d\"\n",
        "\n",
        "MODELS_PATH = './models'\n",
        "LOGS_PATH = './logs'\n",
        "\n",
        "ACT_NEUTRAL = 3 # Added this action to go to neutral and wait\n",
        "ACT_LONG = 2\n",
        "ACT_HOLD = 1\n",
        "ACT_SHORT = 0\n",
        "\n",
        "CAPITAL = 100000\n",
        "TRADE_COSTS_PERCENT = 0.1 / 100\n",
        "\n",
        "FEATURES = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
        "TARGET_FEATURE = \"Price Raw\"\n",
        "STATE_LEN = 30\n",
        "OBS_SPACE = (STATE_LEN)*len(FEATURES)\n",
        "ACT_SPACE = 2\n",
        "\n",
        "BATCH_SIZE = OBS_SPACE * 5\n",
        "LEARN_RATE = 1e-3\n",
        "TOTAL_ITERS = 10000\n",
        "TRAIN_EPISODES = 10\n",
        "INIT_COLLECT = 100\n",
        "TOTAL_COLLECT = 1\n",
        "LOG_INTERVALS = 20\n",
        "TEST_INTERVALS = 100\n",
        "MEMORY_LENGTH = OBS_SPACE * 100\n",
        "DISCOUNT = 0.4\n",
        "EPSILON_START = 1.\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 10000\n",
        "GRAD_CLIP = 1\n",
        "REWARD_CLIP = 1\n",
        "\n",
        "DROPOUT = 0.2\n",
        "L2FACTOR = 0.000001\n",
        "NEURONS = 512\n",
        "LAYERS = (NEURONS, NEURONS//2, NEURONS//4, max(ACT_SPACE * 2, NEURONS//8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4DpIfIPH0Ch"
      },
      "source": [
        "# Financial Data\n",
        "\n",
        "We download some financial data, this is now the standard in our articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "s64pmt9mH0Cj",
        "outputId": "0d02e203-f6bf-49c9-87a1-0b5aa946b995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSLA => min_date: 2017-01-03 00:00:00, max_date: 2019-12-30 00:00:00, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Open       High        Low      Close  Adj Close     Volume\n",
              "Date                                                                        \n",
              "2017-01-03  14.324000  14.688667  14.064000  14.466000  14.466000   88849500\n",
              "2017-01-04  14.316667  15.200000  14.287333  15.132667  15.132667  168202500\n",
              "2017-01-05  15.094667  15.165333  14.796667  15.116667  15.116667   88675500\n",
              "2017-01-06  15.128667  15.354000  15.030000  15.267333  15.267333   82918500\n",
              "2017-01-09  15.264667  15.461333  15.200000  15.418667  15.418667   59692500\n",
              "...               ...        ...        ...        ...        ...        ...\n",
              "2019-12-23  27.452000  28.134001  27.333332  27.948000  27.948000  199794000\n",
              "2019-12-24  27.890667  28.364668  27.512667  28.350000  28.350000  120820500\n",
              "2019-12-26  28.527332  28.898666  28.423332  28.729334  28.729334  159508500\n",
              "2019-12-27  29.000000  29.020666  28.407333  28.691999  28.691999  149185500\n",
              "2019-12-30  28.586000  28.600000  27.284000  27.646667  27.646667  188796000\n",
              "\n",
              "[753 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-adbb9f03-0273-4aa8-8a94-1a2a38d352d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>14.324000</td>\n",
              "      <td>14.688667</td>\n",
              "      <td>14.064000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>88849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>14.316667</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>14.287333</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>168202500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>15.094667</td>\n",
              "      <td>15.165333</td>\n",
              "      <td>14.796667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>88675500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>15.128667</td>\n",
              "      <td>15.354000</td>\n",
              "      <td>15.030000</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>82918500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>15.264667</td>\n",
              "      <td>15.461333</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>59692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-23</th>\n",
              "      <td>27.452000</td>\n",
              "      <td>28.134001</td>\n",
              "      <td>27.333332</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>199794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-24</th>\n",
              "      <td>27.890667</td>\n",
              "      <td>28.364668</td>\n",
              "      <td>27.512667</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>120820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-26</th>\n",
              "      <td>28.527332</td>\n",
              "      <td>28.898666</td>\n",
              "      <td>28.423332</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>159508500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.020666</td>\n",
              "      <td>28.407333</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>149185500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>28.586000</td>\n",
              "      <td>28.600000</td>\n",
              "      <td>27.284000</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>188796000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>753 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adbb9f03-0273-4aa8-8a94-1a2a38d352d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-adbb9f03-0273-4aa8-8a94-1a2a38d352d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-adbb9f03-0273-4aa8-8a94-1a2a38d352d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b4caf66-470e-4a83-95ae-6c229f73cab1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b4caf66-470e-4a83-95ae-6c229f73cab1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b4caf66-470e-4a83-95ae-6c229f73cab1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tickers[TARGET]\",\n  \"rows\": 753,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-03 00:00:00\",\n        \"max\": \"2019-12-30 00:00:00\",\n        \"num_unique_values\": 753,\n        \"samples\": [\n          \"2019-11-06 00:00:00\",\n          \"2019-08-06 00:00:00\",\n          \"2018-06-25 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0998823778838096,\n        \"min\": 12.073332786560059,\n        \"max\": 29.0,\n        \"num_unique_values\": 711,\n        \"samples\": [\n          20.118667602539062,\n          21.391332626342773,\n          20.06800079345703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1384437712983404,\n        \"min\": 12.445332527160645,\n        \"max\": 29.020666122436523,\n        \"num_unique_values\": 708,\n        \"samples\": [\n          25.796667098999023,\n          21.05466651916504,\n          19.631332397460938\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.052082343134819,\n        \"min\": 11.799332618713379,\n        \"max\": 28.42333221435547,\n        \"num_unique_values\": 721,\n        \"samples\": [\n          23.399999618530273,\n          15.915332794189453,\n          20.600000381469727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0939495816459504,\n        \"min\": 11.9313325881958,\n        \"max\": 28.729333877563477,\n        \"num_unique_values\": 735,\n        \"samples\": [\n          17.89466667175293,\n          18.492666244506836,\n          21.487333297729492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adj Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0939495816459504,\n        \"min\": 11.9313325881958,\n        \"max\": 28.729333877563477,\n        \"num_unique_values\": 735,\n        \"samples\": [\n          17.89466667175293,\n          18.492666244506836,\n          21.487333297729492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66969490,\n        \"min\": 32800500,\n        \"max\": 504745500,\n        \"num_unique_values\": 751,\n        \"samples\": [\n          108093000,\n          89928000,\n          84378000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
        "    tickers = {}\n",
        "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
        "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    for symbol in tickers_symbols:\n",
        "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(cached_file_path):\n",
        "                df = pd.read_parquet(cached_file_path)\n",
        "                df.index = pd.to_datetime(df.index)\n",
        "                assert len(df) > 0\n",
        "            else:\n",
        "                df = yf.download(\n",
        "                    symbol,\n",
        "                    start=START_DATE,\n",
        "                    end=END_DATE,\n",
        "                    progress=False,\n",
        "                    interval=INTERVAL,\n",
        "                )\n",
        "                assert len(df) > 0\n",
        "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
        "            min_date = df.index.min()\n",
        "            max_date = df.index.max()\n",
        "            nan_count = df[\"Close\"].isnull().sum()\n",
        "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
        "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
        "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
        "            print(\n",
        "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
        "            )\n",
        "            tickers[symbol] = df\n",
        "\n",
        "            if min_date > latest_start:\n",
        "                latest_start = min_date\n",
        "            if max_date < earliest_end:\n",
        "                earliest_end = max_date\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {symbol}: {e}\")\n",
        "\n",
        "    return tickers, latest_start, earliest_end\n",
        "\n",
        "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
        "tickers[TARGET]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDY9OBeVsxsA"
      },
      "source": [
        "# The Problem Definition\n",
        "\n",
        "With Q-Training, we shall teach a pavlovian-agent to trade. Our objective is to make sequential interaction that lead to the highest sharpe ratio, formalized by this policy (remember Q-Learning is off-policy, and we won't learn this directly):\n",
        "\n",
        "$$\n",
        "\\pi^*(a_t | s_t) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\middle| s_t = f(o_1, a_1, r_1, \\ldots, o_t), a_t \\right]\n",
        "$$\n",
        "\n",
        "At each timestep *t*:\n",
        "\n",
        "1. Observe the environments state *st* and map history with *f(.)*\n",
        "2. Observations *ot* from history *ht*, have previous actions *a_t-1*, previous observations *o_t-1* and their returns *r_t-1*. For our experiment, we'll encode these into features for a network.\n",
        "3. Execute action *a_t*, which can be: hold, long, short\n",
        "4. Get returns *r_t* discounted at *γt*. *γ* is the discounting factor to prevent the agent from doing only tactical choices for returns in the present (missing better future returns).\n",
        "\n",
        "\n",
        "The *π(at|ht)* creates an action on a Quantity Q *at = Qt*. Where a positive *Q* is the long, the negative *Q* signals a short and when its 0 no action is taken. For this article we will use the definition of policy *π(at|ht)* and Q-Value *Q(at,st)* interchangeably, as Q will define quantities bought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7yPL6IJtuuz"
      },
      "source": [
        "## Observations and State Space\n",
        "\n",
        "The paper uses only High, Low, Open, Close and Volume as observations from the agent's environment state.\n",
        "\n",
        "We will augment this space with 2 technical indicators and 2 macroeconomic indicators:\n",
        "- 20day slow and 7 day fast exponential moving average, from our article: \"Momentum and Reversion Trading Signals Analysis\"\n",
        "- The daily VIX as proxy for market volatility & fear, and the 2 year T-note as proxy for inflation & rates, from our article: \"Temporal Convolutional Neural Network with Conditioning for Broad Market Signals\"\n",
        "\n",
        "$$\n",
        "o_t = s_t \\in \\{{\\text{High}_t}, {\\text{Low}_t}, {\\text{Open}_t}, {\\text{Close}_t}, {\\text{Volume}_t}, {\\text{FastEMA}_t}, {\\text{SlowEMA}_t}, {\\text{VIX}_t}, {\\text{T2YR}_t}\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUoFMk_AsxsA"
      },
      "source": [
        "## Actions and Rewards\n",
        "\n",
        "A core concept in RL is rewards engineering. Let's look at our action space *A* at time *t*:\n",
        "\n",
        "$$\n",
        "a_t = Q_t \\in \\{Q_{\\text{Long}, t}, Q_{\\text{Short}, t}\\}\n",
        "$$\n",
        "\n",
        "The action *Q_Long,t* is set to maximize returns on a buy, given our liquidity *vc_t* (the value *v* of our portfolio with cash remainng *c*) and purchasing *Q_long* at price *p* shares (transaction costs *C*) if we are not already long:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Long}, t} =\n",
        "\\begin{cases}\n",
        "\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Long}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The action *Q_Short,t* aims to convert a **negative** number of shares to returns (shorting is the borrowing of shares, therefore our *v_c* will be initially negative).\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} =\n",
        "\\begin{cases}\n",
        "-2n_t - \\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Short}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Note the *-2n* is an indication to sell twice, meaning not only close the long position but open a short position for the *Qn* shares, since shorting is a negative trajectory, we need to negate the amount we can buy to get the correct representation in our holdings. If we had no shares to start, then *-2(0)* will not have an effect save for the short amount:\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} = -\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)} \\right\\rfloor\n",
        "$$\n",
        "\n",
        "Shorts are risky, and we need to give boundaries to the agent, as a short can incur infinite loss:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Short}, t} = \\max\\{\\hat{Q}_{\\text{Short}, t}, Q_t\\}\n",
        "$$\n",
        "\n",
        "Given that our portfolio cannot fall into negative amounts, we need to model constraints.\n",
        "1. Cash value *vc_t* needs to be large enough to return to neutral *n_t=0*.\n",
        "2. To return to 0, we need to adjust for costs *C* which are caused by market volatility epsiloc *ϵ* (think slippages, spreads, etc..).\n",
        "3. We redefine the action space permissable to ensure we can always return to neutral.\n",
        "\n",
        "$$\n",
        "v_{c,t+1} \\geq -n_{t+1} p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "The action space *A* is redefined as a set of acceptable values for *Q_t* between boundaries *Q-* and *Q+*:\n",
        "\n",
        "$$\n",
        "A = \\left\\{ Q_t \\in \\mathbb{Z} \\cap \\left[Q_t^-, Q_t^+\\right] \\right\\}\n",
        "$$\n",
        "\n",
        "Where the top boundary *Q+* is:\n",
        "$$\n",
        "Q_t^+ = \\frac{v_{c,t}}{p_t (1+C)}\n",
        "$$\n",
        "\n",
        "And the lower boundary *Q-* is (for both coming out of a long where delta *t* is positive, or reversing a short and incurring twice the costs with delta *t* in the negative):\n",
        "\n",
        "$$\n",
        "Q_t^- = \\begin{cases}\n",
        "    \\frac{\\Delta t}{p_t \\varepsilon (1 + C)} & \\text{if } \\Delta t \\geq 0, \\\\\n",
        "    \\frac{\\Delta t}{p_t (2C + \\varepsilon(1 + C))} & \\text{if } \\Delta t < 0,\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "with *delta t* being the in change of portfolio value in time:\n",
        "\n",
        "$$\n",
        " t_Δ = -v_{c,t} - n_t p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "In the above boundaries, the cost of trading is defined as:\n",
        "\n",
        "$$\n",
        "v_{c,t+1} = v_{c,t} - Q_t p_t - C |Q_t| p_t\n",
        "$$\n",
        "\n",
        "Where *C* is the percentage cost of the transaction given the absolute quantity *|Q_t|* of shares and their price *p_t*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A2EBYR9sxsA"
      },
      "source": [
        "## Agent's Objective\n",
        "\n",
        "In the paper, they utilize the percentage returns as a rewards signal, clipped between -1 and 1, and adjusted by a discount factor γ:\n",
        "$$\n",
        "\\text{Signal} = \\left( \\frac{vc_{t} - vc_{t-1}}{vc_{t-1}} \\right) \\gamma^t\n",
        "$$\n",
        "\n",
        "In the article, we will use an annualized Sharpe (from *N* time window, up to 252 trading days), and teach the agent's to generate an optimal ratio, clipped no discount factor:\n",
        "\n",
        "$$\n",
        "\\text{Signal} = \\frac{E\\left[\\sum_{t=0}^{T} r_t - R_f\\times \\right]\\sqrt{N} }{\\sqrt{\\mathrm{Var}\\left[\\sum_{t=0}^{T} r_t\\right]}}\n",
        "$$\n",
        "\n",
        "which is just the maximization of:\n",
        "\n",
        "$$\n",
        "\\text{sharpe}= \\left( \\frac{\\bar{R} - R_f}{\\sigma} \\right)\n",
        "$$\n",
        "\n",
        "or the returns of the portfolio (*R* average), minus the risk free rate (*Rf*, at the time of writing, 5%) divided by the volatility (*σ*) of the portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lke4koO5H0Cl"
      },
      "source": [
        "# Trading Environment\n",
        "\n",
        "Using TensorFlow's PyEnvironment, we will give the agent the environment that implements the above rules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXLhk-CKH0Co"
      },
      "outputs": [],
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
        "\n",
        "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
        "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the stock market data.\n",
        "    - data_dim: Dimension of the data to be used for each observation.\n",
        "    - money: Initial capital to start trading.\n",
        "    - state_length: Number of past observations to consider for the state.\n",
        "    - transaction_cost: Costs associated with trading actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, features = FEATURES, money=CAPITAL, state_length=STATE_LEN, transaction_cost=0, market_costs=TRADE_COSTS_PERCENT, reward_discount=DISCOUNT):\n",
        "        super(TradingEnv, self).__init__()\n",
        "\n",
        "        assert data is not None\n",
        "\n",
        "        self.features = features\n",
        "        self.data_dim = len(self.features)\n",
        "        self.state_length = state_length\n",
        "        self.current_step = self.state_length\n",
        "        self.reward_discount = reward_discount\n",
        "\n",
        "        self.balance = money\n",
        "        self.initial_balance = money\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.epsilon = max(market_costs, np.finfo(float).eps) # there is always volatility costs\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self._episode_ended = False\n",
        "        self._batch_size = 1\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.state_length * self.data_dim, ), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.data = self.preprocess_data(data.copy())\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def batched(self):\n",
        "        return False #True\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return None #self._batch_size\n",
        "\n",
        "    @batch_size.setter\n",
        "    def batch_size(self, size):\n",
        "        self._batch_size = size\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        def _log_rets(df):\n",
        "            log_returns = np.log(df / df.shift(1))\n",
        "            df = (log_returns - log_returns.mean()) / log_returns.std()\n",
        "            df = df.dropna()\n",
        "            return df\n",
        "\n",
        "        price_raw = df['Close'].copy()\n",
        "        df[self.features] = _log_rets(df[self.features])\n",
        "        l = df[self.features]\n",
        "        df = df.replace(0.0, np.nan)\n",
        "        df = df.interpolate(method='linear', limit=5, limit_area='inside')\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        df[TARGET_FEATURE] = price_raw\n",
        "        df['Sharpe'] = 0\n",
        "        df['Position'] = 0\n",
        "        df['Action'] = ACT_HOLD\n",
        "        df['Holdings'] = 0.\n",
        "        df['Cash'] = float(self.balance)\n",
        "        df['Money'] = df['Holdings'] + df['Cash']\n",
        "        df['Returns'] = 0.\n",
        "\n",
        "        assert not df.isna().any().any()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Provides the specification of the action space.\"\"\"\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_step = self.state_length\n",
        "        self._episode_ended = False\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self.data['Sharpe'] = 0\n",
        "        self.data['Position'] = 0\n",
        "        self.data['Action'] = ACT_HOLD\n",
        "        self.data['Holdings'] = 0.\n",
        "        self.data['Cash']  = float(self.balance)\n",
        "        self.data['Money'] = self.data.iloc[0]['Holdings'] + self.data.iloc[0]['Cash']\n",
        "        self.data['Returns'] = 0.\n",
        "\n",
        "        initial_observation = self._next_observation()\n",
        "        return ts.restart(initial_observation)\n",
        "\n",
        "    def _next_observation(self):\n",
        "        \"\"\"Generates the next observation based on the current step and history length.\"\"\"\n",
        "        start_idx = max(0, self.current_step - self.state_length + 1)\n",
        "        end_idx = self.current_step + 1\n",
        "        obs = self.data[self.features].iloc[start_idx:end_idx]\n",
        "\n",
        "        # flatten because: https://stackoverflow.com/questions/67921084/dqn-agent-issue-with-custom-environment\n",
        "        obs_values = obs.values.flatten().astype(np.float32)\n",
        "        return obs_values\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Executes a trading action and updates the environment's state.\"\"\"\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        self.current_step += 1\n",
        "        current_price = self.data.iloc[self.current_step][TARGET_FEATURE]\n",
        "\n",
        "        assert not self.data.iloc[self.current_step].isna().any().any()\n",
        "\n",
        "        if action == ACT_LONG:\n",
        "            self._process_long_position(current_price)\n",
        "        elif action == ACT_SHORT:\n",
        "            prev_current_price = self.data.iloc[self.current_step - 1][TARGET_FEATURE]\n",
        "            self._process_short_position(current_price, prev_current_price)\n",
        "        elif action == ACT_HOLD:\n",
        "            self._process_hold_position()\n",
        "        elif action == ACT_NEUTRAL:\n",
        "            self._process_neutral_position(current_price)\n",
        "        else:\n",
        "          raise Exception(f\"Invalid Actions: {action}\")\n",
        "\n",
        "        self._update_financials()\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        reward = self._calculate_reward_signal()\n",
        "        if done:\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(self._next_observation(), reward)\n",
        "        else:\n",
        "            return ts.transition(self._next_observation(), reward, discount=self.reward_discount)\n",
        "\n",
        "    def _get_lower_bound(self, cash, total_shares, price):\n",
        "        \"\"\"\n",
        "        Compute the lower bound of the action space, particularly for short selling,\n",
        "        based on current cash, the number of shares, and the current price.\n",
        "        \"\"\"\n",
        "        delta = -cash - total_shares * price * (1 + self.epsilon) * (1 + self.transaction_cost)\n",
        "\n",
        "        if delta < 0:\n",
        "            lowerBound = delta / (price * (2 * self.transaction_cost + self.epsilon * (1 + self.transaction_cost)))\n",
        "        else:\n",
        "            lowerBound = delta / (price * self.epsilon * (1 + self.transaction_cost))\n",
        "\n",
        "        if np.isinf(lowerBound):\n",
        "            assert False\n",
        "        return lowerBound\n",
        "\n",
        "    def _process_hold_position(self):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "        self.data.at[step_idx, \"Holdings\"] = self.data.iloc[self.current_step - 1][\"Holdings\"]\n",
        "        self.data.at[step_idx, \"Position\"] = self.data.iloc[self.current_step - 1][\"Position\"]\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
        "\n",
        "    def _process_neutral_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "        self.data.at[step_idx, \"Holdings\"] = 0.0\n",
        "        self.data.at[step_idx, \"Position\"] = 0.0\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_NEUTRAL\n",
        "\n",
        "    def _process_long_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = 1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == 1:\n",
        "            # more long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash']\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new long\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "        else:\n",
        "            # short to long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "\n",
        "    def _process_short_position(self, current_price, prev_price):\n",
        "        \"\"\"\n",
        "        Adjusts the logic for processing short positions to include lower bound calculations.\n",
        "        \"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = -1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == -1:\n",
        "            # Short more\n",
        "            low = self._get_lower_bound(self.data.iloc[self.current_step - 1]['Cash'], -self.total_shares, prev_price)\n",
        "            if low <= 0:\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            else:\n",
        "                total_sharesToBuy = min(math.floor(low), self.total_shares)\n",
        "                self.total_shares -= total_sharesToBuy\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] - total_sharesToBuy * current_price * (1 + self.transaction_cost)\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new short\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "        else:\n",
        "            # long to short\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "\n",
        "    def _update_financials(self):\n",
        "        \"\"\"Updates the financial metrics including cash, money, and returns.\"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.balance = self.data.iloc[self.current_step]['Cash']\n",
        "\n",
        "        self.data.at[step_idx,'Money'] = self.data.iloc[self.current_step]['Holdings'] + self.data.iloc[self.current_step]['Cash']\n",
        "        self.data.at[step_idx,'Returns'] = ((self.data.iloc[self.current_step]['Money'] - self.data.iloc[self.current_step - 1]['Money'])) / self.data.iloc[self.current_step - 1]['Money']\n",
        "\n",
        "    def _calculate_reward_signal(self, reward_clip=REWARD_CLIP):\n",
        "        \"\"\"\n",
        "        Calculates the reward for the current step. In the paper they use the %returns.\n",
        "        \"\"\"\n",
        "        return np.clip(self.data.iloc[self.current_step]['Returns'], -reward_clip, reward_clip)\n",
        "\n",
        "    def _calculate_sharpe_reward_signal(self, risk_free_rate=0.05, periods_per_year=252, reward_clip=REWARD_CLIP):\n",
        "        \"\"\"\n",
        "        Calculates the annualized Sharpe ratio up to the CURRENT STEP.\n",
        "\n",
        "        Parameters:\n",
        "        - risk_free_rate (float): The annual risk-free rate. It will be adjusted to match the period of the returns.\n",
        "        - periods_per_year (int): Number of periods in a year (e.g., 252 for daily, 12 for monthly).\n",
        "\n",
        "        Returns:\n",
        "        - float: The annualized Sharpe ratio as reward.\n",
        "        \"\"\"\n",
        "        period_risk_free_rate = (1 + risk_free_rate) ** (1 / periods_per_year) - 1\n",
        "        observed_returns = self.data['Returns'].iloc[:self.current_step + 1]\n",
        "\n",
        "        excess_returns = observed_returns - period_risk_free_rate\n",
        "\n",
        "        mean_excess_return = np.mean(excess_returns)\n",
        "        std_dev_returns = np.std(observed_returns)\n",
        "\n",
        "        sharpe_ratio = mean_excess_return / std_dev_returns if std_dev_returns > 0 else 0\n",
        "        annual_sr = sharpe_ratio * np.sqrt(periods_per_year)\n",
        "\n",
        "        self.data.at[self.data.index[self.current_step], 'Sharpe'] = annual_sr\n",
        "\n",
        "        return np.clip(annual_sr, -reward_clip, reward_clip)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f'Step: {self.current_step}, Balance: {self.balance}, Holdings: {self.total_shares}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2lK8WKhtuu4",
        "outputId": "86da4706-dc4b-4f20-8045-197c268d128d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep Specs: TimeStep(\n",
            "{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n",
            " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
            " 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
            " 'observation': BoundedArraySpec(shape=(150,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)})\n",
            "Action Specs: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n",
            "Reward Specs: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ],
      "source": [
        "stock= tickers[TARGET]\n",
        "train_data = stock[stock.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
        "test_data = stock[stock.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
        "\n",
        "train_env = TradingEnv(train_data)\n",
        "utils.validate_py_environment(train_env, episodes=TRAIN_EPISODES)\n",
        "test_env = TradingEnv(test_data)\n",
        "utils.validate_py_environment(train_env, episodes=TRAIN_EPISODES//4)\n",
        "\n",
        "print(f\"TimeStep Specs: {train_env.time_step_spec()}\")\n",
        "print(f\"Action Specs: {train_env.action_spec()}\")\n",
        "print(f\"Reward Specs: {train_env.time_step_spec().reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yXdVTRctuu5",
        "outputId": "1e7b522e-dcb0-4c47-a822-617267fb1725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: 1 at step: 31\n",
            "New balance: 100000.0\n",
            "Total shares: 0\n",
            "Reward: 0.0\n",
            "\n",
            "Action taken: 2 at step: 32\n",
            "New balance: 0.848236083984375\n",
            "Total shares: 5510\n",
            "Reward: 0.0\n",
            "\n",
            "Action taken: 0 at step: 33\n",
            "New balance: 203790.03025054932\n",
            "Total shares: 5510\n",
            "Reward: 0.01895439252257347\n",
            "\n",
            "Action taken: 1 at step: 34\n",
            "New balance: 203790.03025054932\n",
            "Total shares: 5510\n",
            "Reward: 0.0\n",
            "\n",
            "Action taken: 2 at step: 35\n",
            "New balance: 4.924432754516602\n",
            "Total shares: 6431\n",
            "Reward: 0.07714703679084778\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def execute_action_and_print_state(env, action):\n",
        "    next_time_step = env.step(np.array(action, dtype=np.int32))\n",
        "    print(f'Action taken: {action} at step: {env.current_step}')\n",
        "    # print(f'New state: {next_time_step.observation}')\n",
        "    print(f'New balance: {env.balance}')\n",
        "    print(f'Total shares: {env.total_shares}')\n",
        "    print(f'Reward: {next_time_step.reward}\\n')\n",
        "\n",
        "time_step = train_env.reset()\n",
        "# print(f'Initial state: {time_step.observation}')\n",
        "\n",
        "# Some dryruns to validate our env logic: Buy, Sell, we should have a positive balance with TSLA\n",
        "execute_action_and_print_state(train_env, ACT_HOLD)\n",
        "execute_action_and_print_state(train_env, ACT_LONG)\n",
        "execute_action_and_print_state(train_env, ACT_SHORT)\n",
        "execute_action_and_print_state(train_env, ACT_HOLD)\n",
        "execute_action_and_print_state(train_env, ACT_LONG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcb7hq6yH0Cq"
      },
      "source": [
        "# Deep Q-Network Architecure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIE_vPrH0Cr"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "Deep Q-neural-network architecture (DQN) approximates the the Q-tables algorithm as its approximating the action-value function π∗(at|st). Its an approximation because the number of combinations you can have with your Q-Tables is gargantuan and impossible to process.\n",
        "\n",
        "The Q-network is also referred to as the policy model. We will also leverage a target Q-network in our architecture. Tha Target model is updated more seldomly than the Q-Network, and helps stabilize the training process as the Q-Network is trained to reduced its output and the target network (a more stable value).\n",
        "\n",
        "Finally, we will add a Replay Memory to sample data for our models. The memory is a circular memory of fixed size (therefore it 'forgets' old memories), and at every fixed frequency the models use the memory to calculate the loss between their predicted Q values and the ones performed in the memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q49V1zseH0Cs"
      },
      "source": [
        "## The Reinforce Learning Flow\n",
        "\n",
        "A picture says a thousand words; the flow chart below will guide us on the whole training and updating the target model:\n",
        "\n",
        "![\"Flowchart of training\"](https://raw.githubusercontent.com/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/main/images/Q-net.png)\n",
        "\n",
        "First we initinialize the environment *St* and the action state *Qt*.\n",
        "\n",
        "We then run multiple episodes of n iterations to train the model and remember the state, actions and the Q value predicted. On each iteration, these events will occur:\n",
        "1. Fetch the state.\n",
        "1. Take either a random action (ε greedy) or predict a Q value given an action in the current state, the former is called exploration, and the latter exploitation. The ε decays with time, as the model learns, it should explore less.\n",
        "1. When predicting the Q value, it will use the policy model. Regardles of exploring or explointing, it saves the memory of the states, actions and the given Q value.\n",
        "1. Calculate the target Q-Value by taking the max predictions from the target-network. From the previous formula *rt + γt * Qtarget(s_t+1, a_t+1)* where gamma *γ* is the discounting factor.\n",
        "1. Re-traing the policy model to minimize the Q-values from the different models. Training uses sampled states from our replay memory.\n",
        "1. At the end of the episode, or any interval of our chosing, we copy the wieghts of the policy model to the target model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y5vgYpxH0Cs",
        "outputId": "5fb62fff-31aa-4fa3-e518-55ad5786f4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "<tf_agents.policies.greedy_policy.GreedyPolicy object at 0x79958ab08d60>\n",
            "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7995ef6f1c90>\n"
          ]
        }
      ],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def create_q_network(env, fc_layer_params=LAYERS, dropout_rate=DROPOUT, l2_reg=L2FACTOR):\n",
        "    \"\"\"\n",
        "    Creates a Q-Network with dropout and batch normalization.\n",
        "    Parameters:\n",
        "    - env: The environment instance.\n",
        "    - fc_layer_params: Tuple of integers representing the number of units in each dense layer.\n",
        "    - dropout_rate: Dropout rate for dropout layers.\n",
        "    - l2_reg: L2 regularization factor.\n",
        "\n",
        "    Returns:\n",
        "    - q_net: The Q-Network model.\n",
        "    \"\"\"\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "    action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "    layers = []\n",
        "    for num_units in fc_layer_params:\n",
        "        layers.append(tf.keras.layers.Dense(\n",
        "                                num_units,\n",
        "                                activation=None,\n",
        "                                kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'),\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
        "        layers.append(tf.keras.layers.BatchNormalization())\n",
        "        layers.append(tf.keras.layers.LeakyReLU())\n",
        "        layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "    q_values_layer = tf.keras.layers.Dense(\n",
        "        num_actions,\n",
        "        activation=None,\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        bias_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
        "\n",
        "    q_net = sequential.Sequential(layers + [q_values_layer])\n",
        "\n",
        "    return q_net\n",
        "\n",
        "def create_agent(q_net, env, t_q_net = None, optimizer = None, eps=EPSILON_START, learning_rate=LEARN_RATE, gradient_clipping = GRAD_CLIP):\n",
        "    \"\"\"\n",
        "    Creates a DQN agent for a given environment with specified configurations.\n",
        "\n",
        "    Parameters:\n",
        "    - q_net (tf_agents.networks.Network): The primary Q-network for the agent.\n",
        "    - env (tf_agents.environments.PyEnvironment or tf_agents.environments.TFPyEnvironment):\n",
        "      The environment the agent will interact with. A TFPyEnvironment wrapper is applied\n",
        "      if not already wrapped.\n",
        "    - t_q_net (tf_agents.networks.Network, optional): The target Q-network for the agent.\n",
        "      If None, no target network is used.\n",
        "    - optimizer (tf.keras.optimizers.Optimizer, optional): The optimizer to use for training the agent.\n",
        "      If None, an Adam optimizer with exponential decay learning rate is used.\n",
        "    - eps (float): The epsilon value for epsilon-greedy exploration.\n",
        "    - learning_rate (float): The initial learning rate for the exponential decay learning rate schedule.\n",
        "      Ignored if an optimizer is provided.\n",
        "    - gradient_clipping (float): The value for gradient clipping. If 1., no clipping is applied.\n",
        "\n",
        "    Returns:\n",
        "    - agent (tf_agents.agents.DqnAgent): The initialized and configured DQN agent.\n",
        "    \"\"\"\n",
        "    if optimizer is None:\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "    # see: https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DqnAgent\n",
        "    agent = dqn_agent.DqnAgent(\n",
        "        env.time_step_spec(),\n",
        "        env.action_spec(),\n",
        "        q_network=q_net,\n",
        "        target_q_network = t_q_net,\n",
        "        optimizer=optimizer,\n",
        "        epsilon_greedy = eps,\n",
        "        reward_scale_factor = 1,\n",
        "        gradient_clipping = gradient_clipping,\n",
        "        td_errors_loss_fn=common.element_wise_huber_loss,\n",
        "        train_step_counter=tf.compat.v1.train.get_or_create_global_step(),\n",
        "        name=\"TradeAgent\")\n",
        "\n",
        "    agent.initialize()\n",
        "    print(agent.policy)\n",
        "    print(agent.collect_policy)\n",
        "    return agent\n",
        "\n",
        "q_net = create_q_network(train_env)\n",
        "t_q_net = create_q_network(train_env)\n",
        "agent = create_agent(q_net, train_env, t_q_net=t_q_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZ_bo04H0Ct"
      },
      "source": [
        "# Trading Operations\n",
        "\n",
        "Using TensorFlow agents' framework, training our pavlovian trader should be easier than building the architecture ourselves.\n",
        "\n",
        "The trading simulator class will prepare all the variables required. In this case it will initialize the reply memory using DeepMind's Reverb, and create a collector policy for the agent. Unlike the evaluation policy (*π(at|ht)*) which is use to predict the target Q value, the collector will explore and collect data with actions and their resulting value for the memory, memories are saved as trajectories (*τ*) in tensorflow which is a collection of the current observed state (*ot*), the action taken (*at*), the reward received (*r_t+1*) and the following observed state (*o_t+1*) formalized as *r=(o_t-1, a_t-1, rt, ot, dt)*, where dt is a flag for the end state if this was the last observation.\n",
        "\n",
        "To give learning opportunity to our agent, we will use a high epsilon to have it explore a lot, and slowly decay it using the formula below:\n",
        "\n",
        "$$\n",
        "\\epsilon_{\\text{decayed}} = \\epsilon_{\\text{final}} + (\\epsilon_{\\text{initial}} - \\epsilon_{\\text{final}}) \\times e^{-\\frac{\\text{step}}{\\text{decay\\_steps}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- *ϵ_decayed* is the decayed epsilon value at the current step,\n",
        "- *ϵ_initial* is the initial epsilon value at the start of training, we set it to 1, meaning it only explores at start.\n",
        "- *ϵ_final* is the end value we want that the agent exploits is environment, preferably when deployed.\n",
        "- *step* is the current step or iteration in the training process, and decay_steps is a parameter that controls the rate, in our case 1000. As the steps approach the end, the decay will get smaller and smaller.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DmbtWugH0Ct"
      },
      "outputs": [],
      "source": [
        "class TradingSimulator:\n",
        "    def __init__(self, env, eval_env, agent, episodes=TRAIN_EPISODES,\n",
        "                 batch_size=BATCH_SIZE, num_eval_episodes=TEST_INTERVALS,\n",
        "                 collect_steps_per_iteration=INIT_COLLECT,\n",
        "                 replay_buffer_max_length=MEMORY_LENGTH ,\n",
        "                 num_iterations = TOTAL_ITERS, log_interval=LOG_INTERVALS,\n",
        "                 eval_interval=TEST_INTERVALS, global_step=None):\n",
        "        self.py_env = env\n",
        "        self.env =  tf_py_environment.TFPyEnvironment(self.py_env)\n",
        "        self.py_eval_env = eval_env\n",
        "        self.eval_env =  tf_py_environment.TFPyEnvironment(self.py_eval_env)\n",
        "        self.agent = agent\n",
        "        self.episodes = episodes\n",
        "        self.log_interval = log_interval\n",
        "        self.eval_interval = eval_interval\n",
        "        self.global_step = global_step\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_eval_episodes = num_eval_episodes\n",
        "        self.collect_steps_per_iteration = collect_steps_per_iteration\n",
        "        self.replay_buffer_max_length = replay_buffer_max_length\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "        self.policy = self.agent.policy\n",
        "        self.collect_policy = self.agent.collect_policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
        "            self.env.time_step_spec(),\n",
        "            self.env.action_spec())\n",
        "\n",
        "        self.replay_buffer_signature = tensor_spec.from_spec(\n",
        "            self.agent.collect_data_spec)\n",
        "        self.replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "            self.replay_buffer_signature)\n",
        "\n",
        "    def init_memory(self, table_name = 'uniform_table'):\n",
        "        self.table = reverb.Table(\n",
        "            table_name,\n",
        "            max_size=self.replay_buffer_max_length,\n",
        "            sampler=reverb.selectors.Uniform(),\n",
        "            remover=reverb.selectors.Fifo(),\n",
        "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "            signature=self.replay_buffer_signature)\n",
        "\n",
        "        self.reverb_server = reverb.Server([self.table])\n",
        "        self.replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "                                    self.agent.collect_data_spec,\n",
        "                                    table_name=table_name,\n",
        "                                    sequence_length=2,\n",
        "                                    local_server=self.reverb_server)\n",
        "\n",
        "        self.rb_observer = reverb_utils.ReverbAddTrajectoryObserver(self.replay_buffer.py_client, table_name, sequence_length=2)\n",
        "        self.dataset = self.replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=self.batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "        return self.dataset, iter(self.dataset)\n",
        "\n",
        "    def compute_episode_metrics(self, environment, policy, num_eval_episodes):\n",
        "        total_returns = []\n",
        "        episode_sharpe_ratios = []\n",
        "        episode_std_devs = []\n",
        "        episode_avg_returns = []\n",
        "\n",
        "        for _ in tqdm(range(num_eval_episodes), desc=f\"compute_episode_metrics for {num_eval_episodes}\"):\n",
        "            time_step = environment.reset()\n",
        "            episode_returns = []\n",
        "\n",
        "            while not time_step.is_last():\n",
        "                action_step = policy.action(time_step)\n",
        "                time_step = environment.step(action_step.action)\n",
        "                rewards = time_step.reward.numpy()\n",
        "                episode_returns.extend(rewards.flatten())\n",
        "\n",
        "            total_episode_return = np.sum(episode_returns)\n",
        "            total_returns.append(total_episode_return)\n",
        "            episode_avg_return = np.mean(episode_returns)\n",
        "            episode_avg_returns.append(episode_avg_return)\n",
        "            episode_std_dev = np.std(episode_returns)\n",
        "            episode_std_devs.append(episode_std_dev)\n",
        "            episode_sharpe_ratio = episode_avg_return / episode_std_dev if episode_std_dev > 0 else 0\n",
        "            episode_sharpe_ratios.append(episode_sharpe_ratio)\n",
        "\n",
        "        return total_returns, episode_avg_returns, episode_std_devs, episode_sharpe_ratios\n",
        "\n",
        "    def train(self, checkpoint_path=MODELS_PATH, initial_epsilon= EPSILON_START, final_epsilon = EPSILON_END, decay_steps=EPSILON_DECAY):\n",
        "        print(\"Preparing replay memory and dataset\")\n",
        "        if IN_COLAB:\n",
        "            print(f\"Saving checkpoint and policies directly to {GDRIVE}\")\n",
        "            checkpoint_path = f\"{GDRIVE}/MyDrive/{checkpoint_path}\"\n",
        "\n",
        "        _, iterator = self.init_memory()\n",
        "\n",
        "        self.global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "        checkpoint_dir = os.path.join(checkpoint_path, 'checkpoint')\n",
        "        train_checkpointer = common.Checkpointer(\n",
        "            ckpt_dir=checkpoint_dir,\n",
        "            max_to_keep=1,\n",
        "            agent=agent,\n",
        "            policy=agent.policy,\n",
        "            replay_buffer=self.replay_buffer,\n",
        "            global_step=self.global_step\n",
        "        )\n",
        "        train_checkpointer.initialize_or_restore()\n",
        "        self.global_step = tf.compat.v1.train.get_global_step()\n",
        "        print(f'Next step restored: {self.global_step}')\n",
        "        self.policy = agent.policy\n",
        "        self.agent.train = common.function(self.agent.train)\n",
        "        self.agent.train_step_counter.assign(self.global_step )\n",
        "        metrics = []\n",
        "        losses = []\n",
        "\n",
        "        time_step = self.py_env.reset()\n",
        "        collect_driver = py_driver.PyDriver(\n",
        "            self.py_env,\n",
        "            py_tf_eager_policy.PyTFEagerPolicy(self.agent.collect_policy, use_tf_function=True),\n",
        "            [self.rb_observer],\n",
        "            max_steps=self.collect_steps_per_iteration)\n",
        "\n",
        "        total_returns, episode_avg_returns, episode_std_devs, episode_sharpe_ratios= self.compute_episode_metrics(self.eval_env, self.policy, 2)\n",
        "        tr = np.mean(total_returns)\n",
        "        av = np.mean(episode_avg_returns)\n",
        "        sr = np.mean(episode_sharpe_ratios)\n",
        "        sd = np.mean(episode_std_devs)\n",
        "        metrics.append([tr, av, sd, sr])\n",
        "\n",
        "        print(f\"Running training starting {self.global_step.numpy()} to {self.num_iterations}\")\n",
        "        remaining_iters = self.num_iterations - self.global_step.numpy()\n",
        "        assert remaining_iters > 0\n",
        "\n",
        "        for _ in tqdm(range(self.global_step.numpy(), remaining_iters), desc=f\"Training for {remaining_iters}\"):\n",
        "            time_step, _ = collect_driver.run(time_step) # Remember\n",
        "            experience, _ = next(iterator)\n",
        "            train_loss = self.agent.train(experience).loss # Recall and train\n",
        "            step = self.agent.train_step_counter.numpy()\n",
        "            losses.append(train_loss.numpy())\n",
        "\n",
        "            if step % self.log_interval == 0:\n",
        "                print(f'step = {step}: loss = {train_loss} -- Saving {self.global_step} Checkpoint')\n",
        "                # Later call: saved_policy = tf.saved_model.load(policy_dir)\n",
        "                train_checkpointer.save(self.global_step)\n",
        "\n",
        "            if step % self.eval_interval == 0:\n",
        "                total_returns, episode_avg_returns, episode_std_devs, episode_sharpe_ratios= self.compute_episode_metrics(self.eval_env, self.agent.policy, self.eval_interval // 2)\n",
        "                tr = np.mean(total_returns)\n",
        "                av = np.mean(episode_avg_returns)\n",
        "                sr = np.mean(episode_sharpe_ratios)\n",
        "                sd = np.mean(episode_std_devs)\n",
        "\n",
        "                print(f'step = {step}: Average Return = {av}, Total Return = {tr}, Avg Sharpe = {sr} -- Saving {self.global_step} Checkpoint')\n",
        "                train_checkpointer.save(self.global_step)\n",
        "                self.zip_directories(checkpoint_path)\n",
        "                metrics.append([tr, av, sd, sr])\n",
        "\n",
        "            # This is epsilon decay\n",
        "            decayed_epsilon = final_epsilon + (initial_epsilon - final_epsilon) * \\\n",
        "                      np.exp(-1. * step / decay_steps)\n",
        "            agent.collect_policy._epsilon =decayed_epsilon\n",
        "\n",
        "        tr, av, _, sr = zip(*metrics)\n",
        "        print(f'\\nTraining completed. Mean Reward: {np.mean(av):.4f}, Mean Totals: {np.mean(tr):.4f}, Mean Loss: {np.mean(losses):.4f}, Mean Sharpe: {np.mean(sr):.4f}')\n",
        "\n",
        "        policy_dir = os.path.join(checkpoint_path, 'policy')\n",
        "        tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "        tf_policy_saver.save(policy_dir)\n",
        "        self.zip_directories(checkpoint_path)\n",
        "        print(\"Policy saved\")\n",
        "\n",
        "        return losses, metrics\n",
        "\n",
        "    def load_and_eval_policy(self, policy_path=MODELS_PATH):\n",
        "        policy_dir = os.path.join(policy_path, 'policy')\n",
        "\n",
        "        self.policy = tf.saved_model.load(policy_dir)\n",
        "        total_returns, avg_return, _, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.policy, self.eval_interval)\n",
        "        print(f'Average Return = {np.mean(avg_return)}, Total Return = {np.mean(total_returns)}, Sharpe = {np.mean(sharpe_ratio)}')\n",
        "\n",
        "        return self.policy, total_returns, avg_return, sharpe_ratio\n",
        "\n",
        "    def clear_directories(self, directories = MODELS_PATH):\n",
        "        try:\n",
        "            if IN_COLAB:\n",
        "                shutil.rmtree(f\"{GDRIVE}/MyDrive/{directories}\")\n",
        "                print(f\"Successfully cleared {GDRIVE}/MyDrive/{directories}\")\n",
        "            shutil.rmtree(directories)\n",
        "            print(f\"Successfully cleared {directories}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error clearing {directories}: {e}\")\n",
        "\n",
        "\n",
        "    def zip_directories(self, directories = MODELS_PATH, output_filename=f'{MODELS_PATH}/model_files'):\n",
        "        \"\"\"\n",
        "        Creates a zip archive containing the specified directories.\n",
        "\n",
        "        Parameters:\n",
        "        - directories: List of paths to directories to include in the archive.\n",
        "        - output_filename: The base name of the file to create, including the path,\n",
        "                        minus any format-specific extension. Default is 'training_backup'.\n",
        "        \"\"\"\n",
        "        if IN_COLAB:\n",
        "            archive_path = shutil.make_archive(f'{GDRIVE}/MyDrive/{directories}', 'zip', root_dir='.', base_dir=directories)\n",
        "            print(f\"Archived {GDRIVE}/MyDrive/{directories}\")\n",
        "        else:\n",
        "            archive_path = shutil.make_archive(output_filename, 'zip', root_dir='.', base_dir=directories)\n",
        "            print(f\"Archived {directories} into {archive_path}\")\n",
        "\n",
        "\n",
        "    def plot_performance(self, loss, metrics):\n",
        "        \"\"\"\n",
        "        Plot the training performance including average returns and Sharpe Ratios on the same plot,\n",
        "        with returns on the left y-axis and Sharpe Ratios on the right y-axis.\n",
        "        \"\"\"\n",
        "        assert len(loss) > 0 and len(metrics) > 0\n",
        "\n",
        "        _, average_returns, _, sharpe_ratios = zip(*metrics)\n",
        "        iterations = range(0, self.num_iterations + 1, self.eval_interval)\n",
        "        iterations = list(iterations)[:len(average_returns)]\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(18, 4))\n",
        "\n",
        "        axs[0].set_xlabel('Iterations')\n",
        "        axs[0].set_ylabel('Average Return')\n",
        "        axs[0].plot(iterations, average_returns, label='Average Return', color=\"blue\")\n",
        "        axs[0].tick_params(axis='y')\n",
        "        axs[0].legend(loc=\"upper right\")\n",
        "\n",
        "        ax12 = axs[0].twinx()\n",
        "        ax12.set_ylabel('Sharpe Ratio')\n",
        "        ax12.plot(iterations, sharpe_ratios, label='Sharpe Ratio', color=\"yellow\")\n",
        "        ax12.tick_params(axis='y')\n",
        "        ax12.legend(loc=\"upper left\")\n",
        "\n",
        "        axs[1].set_xlabel('Iterations')\n",
        "        axs[1].set_ylabel('Loss')\n",
        "        axs[1].plot(range(0, self.num_iterations), loss, label='Loss', color=\"red\")\n",
        "        axs[1].tick_params(axis='y')\n",
        "        axs[1].legend()\n",
        "\n",
        "        fig.tight_layout()\n",
        "        plt.title('Training Performance: Average Returns and Sharpe Ratios')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_returns_and_actions(self):\n",
        "        \"\"\"\n",
        "        Steps through the environment's data using the given policy and plots the actions,\n",
        "        along with a subplot for cumulative returns.\n",
        "        \"\"\"\n",
        "        data = self.py_eval_env.data.copy()\n",
        "        actions = [ACT_HOLD]\n",
        "        cumulative_returns = [0]\n",
        "        time_step = self.eval_env.reset()\n",
        "\n",
        "        for _ in tqdm(range(1, len(data)), desc=f\"Live actions for {len(data)} iters\"):\n",
        "            action_step = self.policy.action(time_step)\n",
        "            time_step = self.eval_env.step(action_step.action)\n",
        "            actions.append(action_step.action)\n",
        "\n",
        "            if len(cumulative_returns) > 1:\n",
        "                cumulative_returns.append(cumulative_returns[-1] + time_step.reward.numpy()[-1] )\n",
        "            else:\n",
        "                cumulative_returns.append(time_step.reward.numpy()[-1])\n",
        "        data['Action'] = actions\n",
        "        data['Cumulative_Returns'] = np.cumsum(cumulative_returns)\n",
        "\n",
        "        fig, axs = plt.subplots(2, 1, figsize=(18, 8), gridspec_kw={'height_ratios': (3, 1)})\n",
        "\n",
        "        axs[0].plot(data.index, data[TARGET_FEATURE], label='Close', color='k', alpha=0.6)\n",
        "        buys = data[data['Action'] == ACT_LONG]\n",
        "        axs[0].scatter(buys.index, buys[TARGET_FEATURE], label='Buy', color='green', marker='^', alpha=1)\n",
        "        sells = data[data['Action'] == ACT_SHORT]\n",
        "        axs[0].scatter(sells.index, sells[TARGET_FEATURE], label='Sell', color='red', marker='v', alpha=1)\n",
        "        axs[0].set_ylabel('Close ($)')\n",
        "        axs[0].set_title('Trading Actions and Close Prices')\n",
        "        axs[0].legend()\n",
        "\n",
        "\n",
        "        axs[1].plot(data.index, data['Cumulative_Returns'], label='Cumulative Returns', color='blue')\n",
        "        axs[1].set_ylabel('Cumulative Returns')\n",
        "        axs[1].set_title('Cumulative Returns')\n",
        "        axs[1].legend()\n",
        "\n",
        "        for ax in axs:\n",
        "            ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        fig.autofmt_xdate()\n",
        "\n",
        "        plt.xlabel('Date')\n",
        "        plt.show()\n",
        "\n",
        "sim = TradingSimulator(train_env, test_env, agent=agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIiKeLLfJ3SG"
      },
      "outputs": [],
      "source": [
        "# Uncomment this for a new run to remove previously saved policies and checkpoints\n",
        "# sim.clear_directories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C8944WF9ykML",
        "outputId": "b27dd69c-2ed4-4b14-958b-e27a40656205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing replay memory and dataset\n",
            "Saving checkpoint and policies directly to /content/drive\n",
            "Next step restored: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1000>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "compute_episode_metrics for 2: 100%|██████████| 2/2 [00:32<00:00, 16.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running training starting 1000 to 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   0%|          | 0/8000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "Training for 9000:   0%|          | 19/8000 [00:24<2:04:17,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1020: loss = 0.004145652987062931 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1020> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   0%|          | 40/8000 [00:45<2:26:49,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1040: loss = 0.004723728634417057 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1040> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   1%|          | 60/8000 [01:05<2:11:07,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1060: loss = 0.004572520963847637 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1060> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   1%|          | 80/8000 [01:24<2:10:54,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1080: loss = 0.004273165017366409 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1080> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   1%|          | 99/8000 [01:43<2:08:58,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1100: loss = 0.004554554354399443 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1100> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:15<12:42, 15.56s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:28<11:22, 14.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:42<10:46, 13.76s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:55<10:20, 13.49s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:08<10:00, 13.35s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:21<09:43, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  14%|█▍        | 7/50 [01:34<09:25, 13.15s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  16%|█▌        | 8/50 [01:47<09:07, 13.05s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  18%|█▊        | 9/50 [02:00<08:56, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  20%|██        | 10/50 [02:14<09:00, 13.52s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  22%|██▏       | 11/50 [02:28<08:45, 13.46s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  24%|██▍       | 12/50 [02:41<08:28, 13.39s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  26%|██▌       | 13/50 [02:54<08:13, 13.34s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  28%|██▊       | 14/50 [03:07<08:00, 13.35s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  30%|███       | 15/50 [03:20<07:42, 13.21s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  32%|███▏      | 16/50 [03:34<07:29, 13.21s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  34%|███▍      | 17/50 [03:47<07:14, 13.17s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  36%|███▌      | 18/50 [04:02<07:26, 13.95s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  38%|███▊      | 19/50 [04:16<07:13, 13.98s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  40%|████      | 20/50 [04:29<06:51, 13.71s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  42%|████▏     | 21/50 [04:43<06:33, 13.57s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  44%|████▍     | 22/50 [04:56<06:19, 13.54s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  46%|████▌     | 23/50 [05:10<06:08, 13.66s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  48%|████▊     | 24/50 [05:23<05:51, 13.54s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  50%|█████     | 25/50 [05:37<05:38, 13.55s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  52%|█████▏    | 26/50 [05:50<05:22, 13.46s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  54%|█████▍    | 27/50 [06:03<05:07, 13.38s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  56%|█████▌    | 28/50 [06:17<04:52, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  58%|█████▊    | 29/50 [06:31<04:49, 13.77s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  60%|██████    | 30/50 [06:45<04:31, 13.60s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  62%|██████▏   | 31/50 [06:58<04:17, 13.55s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  64%|██████▍   | 32/50 [07:12<04:04, 13.57s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  66%|██████▌   | 33/50 [07:25<03:51, 13.62s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  68%|██████▊   | 34/50 [07:39<03:36, 13.50s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  70%|███████   | 35/50 [07:52<03:21, 13.46s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  72%|███████▏  | 36/50 [08:05<03:08, 13.48s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  74%|███████▍  | 37/50 [08:19<02:54, 13.44s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  76%|███████▌  | 38/50 [08:32<02:40, 13.40s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  78%|███████▊  | 39/50 [08:45<02:26, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  80%|████████  | 40/50 [08:58<02:12, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  82%|████████▏ | 41/50 [09:12<01:59, 13.33s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  84%|████████▍ | 42/50 [09:25<01:45, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  86%|████████▌ | 43/50 [09:38<01:32, 13.19s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  88%|████████▊ | 44/50 [09:51<01:18, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  90%|█████████ | 45/50 [10:04<01:05, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  92%|█████████▏| 46/50 [10:17<00:52, 13.11s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  94%|█████████▍| 47/50 [10:32<00:40, 13.61s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  96%|█████████▌| 48/50 [10:45<00:27, 13.50s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  98%|█████████▊| 49/50 [10:58<00:13, 13.38s/it]\u001b[A\n",
            "compute_episode_metrics for 50: 100%|██████████| 50/50 [11:12<00:00, 13.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1100: Average Return = -0.0014619481517001987, Total Return = -0.6885775923728943, Avg Sharpe = -0.0434364378452301 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1100> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   1%|▏         | 100/8000 [12:57<445:23:07, 202.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived /content/drive/MyDrive//content/drive/MyDrive/./models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   2%|▏         | 120/8000 [13:17<2:23:16,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1120: loss = 0.0037440783344209194 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1120> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   2%|▏         | 140/8000 [13:37<2:31:34,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1140: loss = 0.0041010091081261635 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1140> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   2%|▏         | 160/8000 [13:57<2:06:25,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1160: loss = 0.0039247991517186165 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1160> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   2%|▏         | 179/8000 [14:15<2:09:57,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1180: loss = 0.003653370775282383 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1180> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   2%|▏         | 199/8000 [14:35<2:02:17,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1200: loss = 0.003527451306581497 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1200> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:13<10:47, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:26<10:40, 13.35s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:39<10:20, 13.21s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:52<10:03, 13.12s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:05<09:51, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:20<09:58, 13.61s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  14%|█▍        | 7/50 [01:34<09:54, 13.84s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  16%|█▌        | 8/50 [01:49<09:49, 14.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  18%|█▊        | 9/50 [02:02<09:22, 13.73s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  20%|██        | 10/50 [02:15<09:04, 13.61s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  22%|██▏       | 11/50 [02:28<08:43, 13.43s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  24%|██▍       | 12/50 [02:41<08:25, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  26%|██▌       | 13/50 [02:54<08:09, 13.24s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  28%|██▊       | 14/50 [03:07<07:54, 13.19s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  30%|███       | 15/50 [03:20<07:40, 13.17s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  32%|███▏      | 16/50 [03:33<07:24, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  34%|███▍      | 17/50 [03:46<07:12, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  36%|███▌      | 18/50 [03:59<06:56, 13.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  38%|███▊      | 19/50 [04:13<06:46, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  40%|████      | 20/50 [04:26<06:32, 13.09s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  42%|████▏     | 21/50 [04:39<06:18, 13.05s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  44%|████▍     | 22/50 [04:52<06:06, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  46%|████▌     | 23/50 [05:05<05:51, 13.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  48%|████▊     | 24/50 [05:18<05:38, 13.01s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  50%|█████     | 25/50 [05:31<05:26, 13.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  52%|█████▏    | 26/50 [05:45<05:21, 13.40s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  54%|█████▍    | 27/50 [05:58<05:04, 13.23s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  56%|█████▌    | 28/50 [06:11<04:48, 13.12s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  58%|█████▊    | 29/50 [06:24<04:35, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  60%|██████    | 30/50 [06:37<04:21, 13.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  62%|██████▏   | 31/50 [06:50<04:08, 13.05s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  64%|██████▍   | 32/50 [07:03<03:55, 13.11s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  66%|██████▌   | 33/50 [07:16<03:42, 13.11s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  68%|██████▊   | 34/50 [07:29<03:30, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  70%|███████   | 35/50 [07:42<03:17, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  72%|███████▏  | 36/50 [07:56<03:03, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  74%|███████▍  | 37/50 [08:08<02:50, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  76%|███████▌  | 38/50 [08:21<02:36, 13.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  78%|███████▊  | 39/50 [08:34<02:23, 13.00s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  80%|████████  | 40/50 [08:56<02:36, 15.61s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  82%|████████▏ | 41/50 [09:20<02:43, 18.13s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  84%|████████▍ | 42/50 [09:33<02:13, 16.65s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  86%|████████▌ | 43/50 [09:47<01:51, 15.91s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  88%|████████▊ | 44/50 [10:00<01:30, 15.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  90%|█████████ | 45/50 [10:14<01:12, 14.46s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  92%|█████████▏| 46/50 [10:27<00:56, 14.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  94%|█████████▍| 47/50 [10:40<00:41, 13.73s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  96%|█████████▌| 48/50 [10:53<00:27, 13.51s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  98%|█████████▊| 49/50 [11:07<00:13, 13.70s/it]\u001b[A\n",
            "compute_episode_metrics for 50: 100%|██████████| 50/50 [11:21<00:00, 13.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1200: Average Return = -0.0006567828822880983, Total Return = -0.30934473872184753, Avg Sharpe = -0.01865880936384201 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1200> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   2%|▎         | 200/8000 [25:58<445:34:20, 205.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived /content/drive/MyDrive//content/drive/MyDrive/./models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   3%|▎         | 219/8000 [26:16<2:40:03,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1220: loss = 0.0038816442247480154 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1220> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   3%|▎         | 240/8000 [26:38<2:10:41,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1240: loss = 0.003610669868066907 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1240> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   3%|▎         | 260/8000 [26:58<2:23:15,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1260: loss = 0.0034518036991357803 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1260> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   4%|▎         | 280/8000 [27:18<2:06:51,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1280: loss = 0.0038330089300870895 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1280> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   4%|▎         | 299/8000 [27:36<1:54:52,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1300: loss = 0.0033352402970194817 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1300> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:13<10:44, 13.16s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:26<10:26, 13.05s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:39<10:15, 13.09s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:53<10:26, 13.61s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:06<10:06, 13.48s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:20<09:50, 13.42s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  14%|█▍        | 7/50 [01:33<09:32, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  16%|█▌        | 8/50 [01:46<09:13, 13.17s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  18%|█▊        | 9/50 [01:59<08:57, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  20%|██        | 10/50 [02:12<08:42, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  22%|██▏       | 11/50 [02:25<08:29, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  24%|██▍       | 12/50 [02:38<08:16, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  26%|██▌       | 13/50 [02:51<08:03, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  28%|██▊       | 14/50 [03:04<07:50, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  30%|███       | 15/50 [03:17<07:37, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  32%|███▏      | 16/50 [03:30<07:24, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  34%|███▍      | 17/50 [03:43<07:11, 13.08s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  36%|███▌      | 18/50 [03:56<06:57, 13.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  38%|███▊      | 19/50 [04:09<06:44, 13.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  40%|████      | 20/50 [04:23<06:34, 13.13s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  42%|████▏     | 21/50 [04:35<06:18, 13.06s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  44%|████▍     | 22/50 [04:49<06:10, 13.25s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  46%|████▌     | 23/50 [05:03<06:01, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  48%|████▊     | 24/50 [05:16<05:46, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  50%|█████     | 25/50 [05:29<05:31, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  52%|█████▏    | 26/50 [05:42<05:17, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  54%|█████▍    | 27/50 [05:55<05:02, 13.17s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  56%|█████▌    | 28/50 [06:09<04:49, 13.18s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  58%|█████▊    | 29/50 [06:21<04:35, 13.10s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  60%|██████    | 30/50 [06:34<04:20, 13.02s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  62%|██████▏   | 31/50 [06:47<04:06, 12.97s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  64%|██████▍   | 32/50 [07:00<03:53, 13.00s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  66%|██████▌   | 33/50 [07:13<03:41, 13.03s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  68%|██████▊   | 34/50 [07:26<03:28, 13.05s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  70%|███████   | 35/50 [07:40<03:16, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  72%|███████▏  | 36/50 [07:53<03:03, 13.07s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  74%|███████▍  | 37/50 [08:06<02:50, 13.09s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  76%|███████▌  | 38/50 [08:19<02:37, 13.11s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  78%|███████▊  | 39/50 [08:32<02:23, 13.09s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  80%|████████  | 40/50 [08:45<02:11, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  82%|████████▏ | 41/50 [09:00<02:01, 13.50s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  84%|████████▍ | 42/50 [09:13<01:47, 13.39s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  86%|████████▌ | 43/50 [09:26<01:33, 13.38s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  88%|████████▊ | 44/50 [09:39<01:20, 13.36s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  90%|█████████ | 45/50 [09:53<01:06, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  92%|█████████▏| 46/50 [10:06<00:53, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  94%|█████████▍| 47/50 [10:19<00:39, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  96%|█████████▌| 48/50 [10:32<00:26, 13.28s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  98%|█████████▊| 49/50 [10:45<00:13, 13.23s/it]\u001b[A\n",
            "compute_episode_metrics for 50: 100%|██████████| 50/50 [10:59<00:00, 13.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1300: Average Return = -7.970418664626777e-05, Total Return = -0.03754067420959473, Avg Sharpe = -0.002273523947224021 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1300> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   4%|▍         | 300/8000 [38:37<425:27:56, 198.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived /content/drive/MyDrive//content/drive/MyDrive/./models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   4%|▍         | 320/8000 [38:57<2:46:55,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1320: loss = 0.0032529656309634447 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1320> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   4%|▍         | 340/8000 [39:18<2:07:03,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1340: loss = 0.003476109355688095 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1340> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   4%|▍         | 360/8000 [39:38<2:15:10,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1360: loss = 0.003214439610019326 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1360> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   5%|▍         | 380/8000 [39:58<2:11:50,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1380: loss = 0.003301981370896101 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1380> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   5%|▍         | 399/8000 [40:16<1:54:10,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1400: loss = 0.003118653316050768 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1400> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:14<11:59, 14.68s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:28<11:17, 14.12s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:41<10:40, 13.64s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:54<10:18, 13.45s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:07<09:58, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:20<09:42, 13.24s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  14%|█▍        | 7/50 [01:34<09:30, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  16%|█▌        | 8/50 [01:47<09:18, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  18%|█▊        | 9/50 [02:00<09:06, 13.33s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  20%|██        | 10/50 [02:14<08:54, 13.36s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  22%|██▏       | 11/50 [02:27<08:43, 13.41s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  24%|██▍       | 12/50 [02:41<08:29, 13.40s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  26%|██▌       | 13/50 [02:54<08:14, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  28%|██▊       | 14/50 [03:07<07:58, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  30%|███       | 15/50 [03:20<07:45, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  32%|███▏      | 16/50 [03:34<07:32, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  34%|███▍      | 17/50 [03:47<07:22, 13.42s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  36%|███▌      | 18/50 [04:01<07:10, 13.46s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  38%|███▊      | 19/50 [04:16<07:07, 13.80s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  40%|████      | 20/50 [04:29<06:50, 13.68s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  42%|████▏     | 21/50 [04:42<06:34, 13.60s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  44%|████▍     | 22/50 [04:56<06:17, 13.48s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  46%|████▌     | 23/50 [05:09<06:01, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  48%|████▊     | 24/50 [05:22<05:47, 13.38s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  50%|█████     | 25/50 [05:35<05:32, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  52%|█████▏    | 26/50 [05:48<05:18, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  54%|█████▍    | 27/50 [06:02<05:05, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  56%|█████▌    | 28/50 [06:15<04:51, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  58%|█████▊    | 29/50 [06:28<04:39, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  60%|██████    | 30/50 [06:42<04:26, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  62%|██████▏   | 31/50 [06:55<04:12, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  64%|██████▍   | 32/50 [07:08<03:59, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  66%|██████▌   | 33/50 [07:22<03:45, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  68%|██████▊   | 34/50 [07:35<03:32, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  70%|███████   | 35/50 [07:48<03:18, 13.21s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  72%|███████▏  | 36/50 [08:01<03:04, 13.18s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  74%|███████▍  | 37/50 [08:14<02:51, 13.16s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  76%|███████▌  | 38/50 [08:28<02:42, 13.50s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  78%|███████▊  | 39/50 [08:42<02:27, 13.40s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  80%|████████  | 40/50 [08:55<02:12, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  82%|████████▏ | 41/50 [09:08<01:59, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  84%|████████▍ | 42/50 [09:21<01:46, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  86%|████████▌ | 43/50 [09:35<01:33, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  88%|████████▊ | 44/50 [09:49<01:21, 13.51s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  90%|█████████ | 45/50 [10:02<01:07, 13.49s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  92%|█████████▏| 46/50 [10:15<00:53, 13.47s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  94%|█████████▍| 47/50 [10:29<00:40, 13.42s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  96%|█████████▌| 48/50 [10:42<00:26, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  98%|█████████▊| 49/50 [10:55<00:13, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50: 100%|██████████| 50/50 [11:08<00:00, 13.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1400: Average Return = -0.000260712142335251, Total Return = -0.12279541790485382, Avg Sharpe = -0.00764253456145525 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1400> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   5%|▌         | 400/8000 [51:26<425:36:31, 201.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived /content/drive/MyDrive//content/drive/MyDrive/./models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   5%|▌         | 420/8000 [51:45<2:22:26,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1420: loss = 0.0031510302796959877 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1420> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   5%|▌         | 439/8000 [52:04<2:08:53,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1440: loss = 0.003082817420363426 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1440> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   6%|▌         | 460/8000 [52:25<2:06:58,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1460: loss = 0.0032427574042230844 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1460> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   6%|▌         | 479/8000 [52:46<1:55:22,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1480: loss = 0.0030203768983483315 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1480> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   6%|▌         | 499/8000 [53:07<2:01:24,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1500: loss = 0.0030636147130280733 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1500> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:13<10:48, 13.23s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:26<10:34, 13.21s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:39<10:25, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:52<10:08, 13.23s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:06<09:52, 13.17s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:19<09:41, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  14%|█▍        | 7/50 [01:32<09:28, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  16%|█▌        | 8/50 [01:46<09:19, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  18%|█▊        | 9/50 [01:59<09:06, 13.33s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  20%|██        | 10/50 [02:12<08:52, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  22%|██▏       | 11/50 [02:25<08:37, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  24%|██▍       | 12/50 [02:39<08:23, 13.25s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  26%|██▌       | 13/50 [02:52<08:11, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  28%|██▊       | 14/50 [03:05<07:56, 13.24s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  30%|███       | 15/50 [03:18<07:43, 13.24s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  32%|███▏      | 16/50 [03:33<07:41, 13.57s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  34%|███▍      | 17/50 [03:46<07:24, 13.48s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  36%|███▌      | 18/50 [03:59<07:09, 13.41s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  38%|███▊      | 19/50 [04:12<06:53, 13.33s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  40%|████      | 20/50 [04:25<06:38, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  42%|████▏     | 21/50 [04:39<06:23, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  44%|████▍     | 22/50 [04:52<06:11, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  46%|████▌     | 23/50 [05:06<06:04, 13.50s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  48%|████▊     | 24/50 [05:19<05:47, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  50%|█████     | 25/50 [05:32<05:34, 13.38s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  52%|█████▏    | 26/50 [05:46<05:20, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  54%|█████▍    | 27/50 [05:59<05:05, 13.27s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  56%|█████▌    | 28/50 [06:12<04:54, 13.37s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  58%|█████▊    | 29/50 [06:25<04:38, 13.26s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  60%|██████    | 30/50 [06:39<04:25, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  62%|██████▏   | 31/50 [06:52<04:12, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  64%|██████▍   | 32/50 [07:05<03:58, 13.25s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  66%|██████▌   | 33/50 [07:19<03:47, 13.40s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  68%|██████▊   | 34/50 [07:34<03:39, 13.74s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  70%|███████   | 35/50 [07:47<03:26, 13.75s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  72%|███████▏  | 36/50 [08:01<03:10, 13.60s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  74%|███████▍  | 37/50 [08:14<02:55, 13.52s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  76%|███████▌  | 38/50 [08:27<02:41, 13.48s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  78%|███████▊  | 39/50 [08:40<02:26, 13.31s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  80%|████████  | 40/50 [08:54<02:13, 13.35s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  82%|████████▏ | 41/50 [09:07<01:59, 13.30s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  84%|████████▍ | 42/50 [09:20<01:46, 13.28s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  86%|████████▌ | 43/50 [09:33<01:33, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  88%|████████▊ | 44/50 [09:47<01:19, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  90%|█████████ | 45/50 [10:00<01:06, 13.25s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  92%|█████████▏| 46/50 [10:13<00:53, 13.29s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  94%|█████████▍| 47/50 [10:27<00:39, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  96%|█████████▌| 48/50 [10:40<00:26, 13.36s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  98%|█████████▊| 49/50 [10:53<00:13, 13.32s/it]\u001b[A\n",
            "compute_episode_metrics for 50: 100%|██████████| 50/50 [11:06<00:00, 13.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1500: Average Return = -0.0005363329546526074, Total Return = -0.252612829208374, Avg Sharpe = -0.01567470282316208 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1500> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining for 9000:   6%|▋         | 500/8000 [1:04:15<419:17:05, 201.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived /content/drive/MyDrive//content/drive/MyDrive/./models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   6%|▋         | 520/8000 [1:04:37<2:46:02,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1520: loss = 0.002892176155000925 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1520> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   7%|▋         | 540/8000 [1:04:57<2:03:46,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1540: loss = 0.002860815729945898 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1540> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   7%|▋         | 559/8000 [1:05:16<2:15:51,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1560: loss = 0.0029728072695434093 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1560> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   7%|▋         | 580/8000 [1:05:37<2:01:52,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1580: loss = 0.002966146217659116 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1580> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training for 9000:   7%|▋         | 599/8000 [1:05:56<2:06:53,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1600: loss = 0.0027755622286349535 -- Saving <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1600> Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "compute_episode_metrics for 50:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "compute_episode_metrics for 50:   2%|▏         | 1/50 [00:13<10:43, 13.14s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   4%|▍         | 2/50 [00:26<10:29, 13.12s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   6%|▌         | 3/50 [00:39<10:21, 13.22s/it]\u001b[A\n",
            "compute_episode_metrics for 50:   8%|▊         | 4/50 [00:52<10:07, 13.20s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  10%|█         | 5/50 [01:06<09:59, 13.33s/it]\u001b[A\n",
            "compute_episode_metrics for 50:  12%|█▏        | 6/50 [01:23<10:10, 13.89s/it]\n",
            "Training for 9000:   7%|▋         | 599/8000 [1:07:21<13:52:11,  6.75s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4c0461f43644>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-fc3f57ea156d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, checkpoint_path, initial_epsilon, final_epsilon, decay_steps)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mtotal_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_avg_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_std_devs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_sharpe_ratios\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_episode_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_avg_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fc3f57ea156d>\u001b[0m in \u001b[0;36mcompute_episode_metrics\u001b[0;34m(self, environment, policy, num_eval_episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_agents_tf_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    160\u001b[0m       )\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     q_values, policy_state = self._q_network(\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mnetwork_observation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error  # typed-keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 ):\n\u001b[0;32m-> 1136\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/networks/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, network_state, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m           \u001b[0;31m# Does not maintain state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlayer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_network_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 ):\n\u001b[0;32m-> 1136\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         outputs = tf.nn.batch_normalization(\n\u001b[0m\u001b[1;32m    789\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0m_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1480\u001b[0m   \"\"\"\n\u001b[1;32m   1481\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batchnorm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m     \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m       \u001b[0minv\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    809\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    810\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    535\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "losses, metrics = sim.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGJiWVV_tuu7"
      },
      "outputs": [],
      "source": [
        "sim.plot_performance(losses, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM6Lz_mSylZw"
      },
      "outputs": [],
      "source": [
        "policy, total_returns, avg_return, sharpe_ratio = sim.load_and_eval_policy(policy_path=MODELS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYL1ny9Etuu8"
      },
      "outputs": [],
      "source": [
        "sim.plot_returns_and_actions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeb9dL6H0Cu"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this article we have adapted Deep Q-Network (TDQN) algorithm from *Théate, Thibaut and Ernst, Damien (2021)*, using our signals and Tensorflow's Agent framework. Our agent can now determine optimal trading positions (buy, sell, or hold) to maximize our portfolio returns in a simulated environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrMDUgZH0Cv"
      },
      "source": [
        "## References\n",
        "\n",
        "- [A Random Walk Down Wall Street](https://www.amazon.co.uk/Random-Walk-Down-Wall-Street/dp/0393330338)\n",
        "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
        "- [Open Gym AI Github](https://github.com/openai/gym)\n",
        "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
        "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
        "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
        "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
        "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
        "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)\n",
        "- [Momentum and Reversion Trading Signals Analysis](https://medium.com/call-for-atlas/momentum-and-reversion-the-poor-mans-trading-strategies-9b8e1e6d3496)\n",
        "- [Temporal Convolutional Neural Network with Conditioning for Broad Market Signals](https://medium.com/call-for-atlas/temporal-convolutional-neural-network-with-conditioning-for-broad-market-signals-9f0b0426b2b9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgjl92lH0Cv"
      },
      "source": [
        "## Github\n",
        "\n",
        "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
        "\n",
        "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/deep-q-rl-with-algorithmic-trading-policy)\n",
        "\n",
        "Google Collab available [here](https://colab.research.google.com/drive/1FTj65b2DA8oFgvmmjLc0XIwtII32PwcM?usp=sharing)\n",
        "\n",
        "## Media\n",
        "\n",
        "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
        "\n",
        "## CC Licensing and Use\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}